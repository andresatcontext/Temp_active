{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend, layers, models, utils, losses\n",
    "from tensorflow.keras import datasets, Sequential, preprocessing\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "#from models.resnet import ResNet18\n",
    "# Inside my model training code\n",
    "from data_utils import CIFAR10Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10 Training data shape: (50000, 32, 32, 3)\n",
      "CIFAR10 Training label shape (50000, 1)\n",
      "CIFAR10 Test data shape (10000, 32, 32, 3)\n",
      "CIFAR10 Test label shape (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "cifar10_data = CIFAR10Data()\n",
    "x_train, y_train, x_test, y_test = cifar10_data.get_data(subtract_mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inside my model training code\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mafospinat\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/usr/local/lib/python3.6/dist-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.11<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">distinctive-universe-75</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/afospinat/Active%20Learning\" target=\"_blank\">https://wandb.ai/afospinat/Active%20Learning</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/afospinat/Active%20Learning/runs/9ez3azzw\" target=\"_blank\">https://wandb.ai/afospinat/Active%20Learning/runs/9ez3azzw</a><br/>\n",
       "                Run data is saved locally in <code>/workspace/Active Learning/wandb/run-20201125_152638-9ez3azzw</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"Active Learning\")\n",
    "\n",
    "wandb.config.embedding_size = 128\n",
    "\n",
    "wandb.config.margin = 1.0\n",
    "wandb.config.reduction_in_loss = 'mean' # 'none'\n",
    "\n",
    "wandb.config.w_classif_loss = 1.0\n",
    "wandb.config.w_loss_loss = 0\n",
    "\n",
    "wandb.config.batch_size = 128\n",
    "\n",
    "wandb.config.epoch = 50\n",
    "wandb.config.lr = 0.1\n",
    "wandb.config.milestones = [25, 35]\n",
    "#wandb.config.epochl = 40\n",
    "wandb.config.momentum = 0.9\n",
    "wandb.config.wdecay = 5e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.input_shape = x_train.shape[1:]\n",
    "wandb.config.classes_data = len(np.unique(y_train))\n",
    "wandb.config.NUM_TRAIN = len(x_train) # N\n",
    "\n",
    "wandb.config.ADDENDUM = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48999"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "#  Update the labeled dataset via loss prediction-based uncertainty measurement\n",
    "\n",
    "# Randomly sample 10000 unlabeled data points\n",
    "random.shuffle(unlabeled_set)\n",
    "subset = unlabeled_set[:-1]\n",
    "\n",
    "len(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"''\\n##\\n# Learning Loss for Active Learning\\nNUM_TRAIN = 50000 # N\\nNUM_VAL   = 50000 - NUM_TRAIN\\nBATCH     = 128 # B\\nSUBSET    = 10000 # M\\nADDENDUM  = 1000 # K\\n\\nMARGIN = 1.0 # xi\\nWEIGHT = 1.0 # lambda\\n\\nTRIALS = 3\\nCYCLES = 10\\n\\nEPOCH = 200\\nLR = 0.1\\nMILESTONES = [160]\\nEPOCHL = 120 # After 120 epochs, stop the gradient from the loss prediction module propagated to the target model\\n\\nMOMENTUM = 0.9\\nWDECAY = 5e-4\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''''\n",
    "##\n",
    "# Learning Loss for Active Learning\n",
    "NUM_TRAIN = 50000 # N\n",
    "NUM_VAL   = 50000 - NUM_TRAIN\n",
    "BATCH     = 128 # B\n",
    "SUBSET    = 10000 # M\n",
    "ADDENDUM  = 1000 # K\n",
    "\n",
    "MARGIN = 1.0 # xi\n",
    "WEIGHT = 1.0 # lambda\n",
    "\n",
    "TRIALS = 3\n",
    "CYCLES = 10\n",
    "\n",
    "EPOCH = 200\n",
    "LR = 0.1\n",
    "MILESTONES = [160]\n",
    "EPOCHL = 120 # After 120 epochs, stop the gradient from the loss prediction module propagated to the target model\n",
    "\n",
    "MOMENTUM = 0.9\n",
    "WDECAY = 5e-4\n",
    "'''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(wandb.config.NUM_TRAIN))\n",
    "random.shuffle(indices)\n",
    "labeled_set = indices[:wandb.config.ADDENDUM]\n",
    "unlabeled_set = indices[wandb.config.ADDENDUM:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unlabeled_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a labeled dataset by randomly sampling K=ADDENDUM=1,000 data points from the entire dataset.\n",
    "indices = list(range(wandb.config.NUM_TRAIN))\n",
    "random.shuffle(indices)\n",
    "labeled_set = indices[:wandb.config.ADDENDUM]\n",
    "unlabeled_set = indices[wandb.config.ADDENDUM:]\n",
    "\n",
    "train_gen = train_datagen.flow(x_train[labeled_set],\n",
    "                               y_train[labeled_set],\n",
    "                               batch_size=wandb.config.batch_size)\n",
    "\n",
    "\n",
    "dataloaders  = {'train': train_loader, 'test': test_loader}\n",
    "\n",
    "\n",
    "# generate the classifier\n",
    "Classification_with_AL = generate_model(wandb.config)\n",
    "\n",
    "# \n",
    "Classification_with_AL.compile( optimizer=optimizer,\n",
    "                                loss=loss_dict,\n",
    "                                loss_weights=weigths_dict,\n",
    "                                metrics=metrics)\n",
    "\n",
    "# Active learning cycles\n",
    "for cycle in range(CYCLES):\n",
    "\n",
    "\n",
    "    # \n",
    "    Classification_with_AL.fit_generator(mod_data_gen(train_gen), \n",
    "                                         epochs=wandb.config.EPOCH, \n",
    "                                         steps_per_epoch= len(train_gen), \n",
    "                                         validation_data=mod_data_gen(test_gen),\n",
    "                                         validation_steps=len(test_gen),\n",
    "                                         validation_freq=1,\n",
    "                                         callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 32, 32, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[labeled_set].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate model\n",
    "def generate_model(config):\n",
    "\n",
    "    def get_embedding_nets():\n",
    "        return Sequential([layers.GlobalAveragePooling2D(),layers.Dense(config.embedding_size),layers.Activation(\"relu\")])\n",
    "\n",
    "    def get_classifcation_net():\n",
    "        return Sequential([layers.GlobalAveragePooling2D(),layers.Dense(config.classes_data),layers.Activation(\"relu\")])\n",
    "\n",
    "    def get_classifcation_net():\n",
    "        return Sequential([layers.GlobalAveragePooling2D(),layers.Dense(config.classes_data),layers.Activation(\"relu\")])\n",
    "    \n",
    "    # generate the rest of the model\n",
    "    inputs = tf.keras.Input(shape=config.input_shape)\n",
    "    # add backbone\n",
    "    with tf.variable_scope(\"backbone\"):\n",
    "        backbone = ResNet18(input_shape=config.input_shape, weights='imagenet',include_top=False)\n",
    "        x = backbone(inputs)\n",
    "    # make normal classification\n",
    "    with tf.variable_scope(\"classification\"):\n",
    "        classification = get_classifcation_net()(x[0])\n",
    "        #classification = tf.identity(classification,'out_classification')\n",
    "\n",
    "\n",
    "    with tf.variable_scope(\"loss_learning\"):\n",
    "        # generate embeddings for each other output\n",
    "        embeddings_list =[]\n",
    "        for out in x[1:]:\n",
    "            embeddings_list.append(get_embedding_nets()(out))\n",
    "        embedding = layers.Concatenate()(embeddings_list)\n",
    "        #embedding = tf.identity(embedding,'out_embedding')\n",
    "        out_loss = layers.Dense(1)(embedding)\n",
    "        out_loss = layers.Concatenate()([classification,out_loss])\n",
    "        #out_loss = tf.identity(out_loss,'out_loss_learning')\n",
    "\n",
    "    classifier = models.Model(inputs, [classification,embedding,out_loss])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_loss_v0(c_true, c_pred, l_pred, margin=1.0, reduction='mean'):\n",
    "\n",
    "    assert y_pred.shape[0] % 2 == 0, 'the batch size is not even.'\n",
    "    l_pred_r = l_pred[::-1]\n",
    "    assert l_pred.shape == l_pred_r.shape\n",
    "    \n",
    "    l_pred = (l_pred - l_pred_r)[:y_pred.shape[0]//2]\n",
    "    \n",
    "    # c_true is just the classification as the l_true is calculated here\n",
    "    scc = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')\n",
    "    class_loss = scc(c_true,c_pred)\n",
    "    \n",
    "    target = (class_loss - class_loss[::-1])[:class_loss.shape[0]//2]\n",
    "    target = tf.stop_gradient(target)\n",
    "    \n",
    "    one = (2 * tf.math.sign(  tf.clip_by_value( target, 0, 1))) - 1\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        loss = tf.reduce_sum(tf.clip_by_value(margin - one * l_pred, 0,10000))\n",
    "        loss = loss / tf.cast(l_pred.shape[0], tf.float64)  # Note that the size of l_pred is already halved\n",
    "    elif reduction == 'none':\n",
    "        loss = tf.clip_by_value(margin - one * l_pred, 0,10000)\n",
    "    else:\n",
    "        NotImplementedError()\n",
    "    \n",
    "    return loss,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def compute_loss(y_true, y_pred):\n",
    "    margin=1.0\n",
    "    reduction= 'mean'\n",
    "\n",
    "    assert y_pred.shape[0] % 2 == 0, 'the batch size is not even.'\n",
    "    \n",
    "    #assert y_pred.shape == y_pred.flip(0).shape     \n",
    "    # classification prediction \n",
    "    c_pred = y_pred[:,:-1]\n",
    "    # loss prediction\n",
    "    l_pred = y_pred[:,-1]\n",
    "    l_pred_r = l_pred[::-1]\n",
    "    assert l_pred.shape == l_pred_r.shape\n",
    "    \n",
    "    l_pred = (l_pred - l_pred_r)[:y_pred.shape[0]//2]\n",
    "    \n",
    "    # y_true is just the classification as the l_true is calculated here\n",
    "    scc = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')\n",
    "    class_loss = scc(y_true,c_pred)\n",
    "    \n",
    "    target = (class_loss - class_loss[::-1])[:class_loss.shape[0]//2]\n",
    "    target = tf.stop_gradient(target)\n",
    "    \n",
    "    one = (2 * tf.math.sign(  tf.clip_by_value( target, 0, 1))) - 1\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        loss = tf.reduce_sum(tf.clip_by_value(margin - one * l_pred, 0,10000))\n",
    "        loss = loss / tf.cast(l_pred.shape[0], tf.float64)  # Note that the size of l_pred is already halved\n",
    "    elif reduction == 'none':\n",
    "        loss = tf.clip_by_value(margin - one * l_pred, 0,10000)\n",
    "    else:\n",
    "        NotImplementedError()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def Loss_leaning_loss(y_true, y_pred):\n",
    "    margin=1.0\n",
    "    reduction= 'mean'\n",
    "    c_pred = y_pred[:,:-1]\n",
    "    # loss prediction\n",
    "    l_pred = y_pred[:,-1]\n",
    "    l_pred_r = l_pred[::-1]\n",
    "    #assert tf.shape(y_pred) == tf.shape(l_pred_r)\n",
    "\n",
    "    l_pred = (l_pred - l_pred_r)[:y_pred.shape[0]//2]\n",
    "\n",
    "    # y_true is just the classification as the l_true is calculated here\n",
    "    scc = losses.SparseCategoricalCrossentropy(reduction='none')\n",
    "    class_loss = scc(y_true,c_pred)\n",
    "\n",
    "    l_true = (class_loss - class_loss[::-1])[:class_loss.shape[0]//2]\n",
    "    l_true = tf.stop_gradient(l_true)\n",
    "\n",
    "    one = (2 * tf.math.sign(  tf.clip_by_value( l_true, 0, 1))) - 1\n",
    "\n",
    "    if reduction == 'mean':\n",
    "        loss = tf.reduce_sum(tf.clip_by_value(margin - one * l_pred, 0,10000))\n",
    "        loss = tf.math.divide(loss , tf.cast(tf.shape(l_pred)[0], loss.dtype) ) # Note that the size of l_pred is already halved\n",
    "    elif reduction == 'none':\n",
    "        loss = tf.clip_by_value(self.margin - one * l_pred, 0,10000)\n",
    "    else:\n",
    "        NotImplementedError()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_leaning_loss(losses.Loss):\n",
    "    def __init__(self, margin=1.0, reduction='mean', name=\"Learning_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.margin=1.0\n",
    "        self.reduction= 'mean'\n",
    "\n",
    "\n",
    "    def call(self, y_true, y_pred):  \n",
    "\n",
    "        c_pred = y_pred[:,:-1]\n",
    "        # loss prediction\n",
    "        l_pred = y_pred[:,-1]\n",
    "        l_pred_r = l_pred[::-1]\n",
    "        #assert tf.shape(y_pred) == tf.shape(l_pred_r)\n",
    "\n",
    "        l_pred = (l_pred - l_pred_r)[:y_pred.shape[0]//2]\n",
    "\n",
    "        # y_true is just the classification as the l_true is calculated here\n",
    "        scc = losses.SparseCategoricalCrossentropy(reduction='none')\n",
    "        class_loss = scc(y_true,c_pred)\n",
    "\n",
    "        l_true = (class_loss - class_loss[::-1])[:class_loss.shape[0]//2]\n",
    "        l_true = tf.stop_gradient(l_true)\n",
    "\n",
    "        one = (2 * tf.math.sign(  tf.clip_by_value( l_true, 0, 1))) - 1\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = tf.reduce_sum(tf.clip_by_value(self.margin - one * l_pred, 0,10000))\n",
    "            loss = tf.math.divide(loss , tf.cast(tf.shape(l_pred)[0], loss.dtype) ) # Note that the size of l_pred is already halved\n",
    "        elif self.reduction == 'none':\n",
    "            loss = tf.clip_by_value(self.margin - one * l_pred, 0,10000)\n",
    "        else:\n",
    "            NotImplementedError()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResNet18' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-538963904621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# generate the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mClassification_with_AL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-5b319fccdb7e>\u001b[0m in \u001b[0;36mgenerate_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# add backbone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"backbone\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mbackbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# make normal classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ResNet18' is not defined"
     ]
    }
   ],
   "source": [
    "# generate the classifier\n",
    "Classification_with_AL = generate_model(wandb.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Classification_with_AL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-54eb17754158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mClassification_with_AL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Classification_with_AL' is not defined"
     ]
    }
   ],
   "source": [
    "Classification_with_AL.output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Classification_with_AL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4f3fe1cd69f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m loss_dict={Classification_with_AL.output_names[0]:tf.keras.losses.SparseCategoricalCrossentropy(),\n\u001b[0m\u001b[1;32m     13\u001b[0m            Classification_with_AL.output_names[2]:Loss_leaning_loss()}\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Classification_with_AL' is not defined"
     ]
    }
   ],
   "source": [
    "# generate dataloader for train\n",
    "train_datagen = ImageDataGenerator(\n",
    "        width_shift_range=[-2,2],\n",
    "        height_shift_range=[-2,2],\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# generate dataloader for test\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "\n",
    "# losses\n",
    "loss_dict={Classification_with_AL.output_names[0]:tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "           Classification_with_AL.output_names[2]:Loss_leaning_loss()}\n",
    "\n",
    "# weigths for each loss\n",
    "weigths_dict={ Classification_with_AL.output_names[0]:wandb.config.w_classif_loss,\n",
    "               Classification_with_AL.output_names[1]:0, \n",
    "               Classification_with_AL.output_names[2]:wandb.config.w_loss_loss}\n",
    "\n",
    "# metrics to compute\n",
    "metrics={ Classification_with_AL.output_names[0]:tf.keras.metrics.SparseCategoricalAccuracy()}\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.SGD( learning_rate=wandb.config.lr, momentum=wandb.config.momentum)\n",
    "\n",
    "# callback to define a LearningRateScheduler\n",
    "callbacks = []\n",
    "# Change learning rate\n",
    "callbacks.append( tf.keras.callbacks.LearningRateScheduler(scheduler) )\n",
    "# get te values to wandb\n",
    "callbacks.append( WandbCallback()   )\n",
    "# save the best model\n",
    "#callbacks.append( tf.keras.callbacks.ModelCheckpoint( filepath='model.{epoch:02d}-{val_loss:.2f}.h5'), save_freq='5', **kwargs  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataloader for train\n",
    "train_datagen = ImageDataGenerator(\n",
    "        width_shift_range=[-2,2],\n",
    "        height_shift_range=[-2,2],\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# generate dataloader for test\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "def scheduler(epoch):\n",
    "    lr= wandb.config.lr\n",
    "    for i in wandb.config.milestones:\n",
    "        if epoch>i:\n",
    "            lr*=0.1\n",
    "    return lr\n",
    "\n",
    "# get the data to test \n",
    "test_gen = test_datagen.flow(x_test,y_test,batch_size=wandb.config.batch_size,shuffle=False)\n",
    "train_gen = train_datagen.flow(x_train,y_train,batch_size=wandb.config.batch_size)\n",
    "\n",
    "# callback to define a LearningRateScheduler\n",
    "callbacks = []\n",
    "# Change learning rate\n",
    "callbacks.append( tf.keras.callbacks.LearningRateScheduler(scheduler) )\n",
    "# get te values to wandb\n",
    "callbacks.append( WandbCallback()   )\n",
    "\n",
    "\n",
    "def mod_data_gen(generator):\n",
    "    while True:\n",
    "        X,Y = generator.next()\n",
    "        yield X, [Y, Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classification_with_AL.compile(  optimizer=optimizer,\n",
    "                                loss=loss_dict,\n",
    "                                loss_weights=weigths_dict,\n",
    "                                metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classification_with_AL.fit_generator(mod_data_gen(train_gen), \n",
    "                                     epochs=wandb.config.epoch, \n",
    "                                     steps_per_epoch= len(train_gen), \n",
    "                                     validation_data=mod_data_gen(test_gen),\n",
    "                                     validation_steps=len(test_gen),\n",
    "                                     callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
