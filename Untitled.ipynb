{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load defaulft config\n",
    "import yaml\n",
    "\n",
    "config_path = './configs/Datanet_test.yml'\n",
    "\n",
    "with open(config_path) as file:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Active_Learning_train:\n",
    "    def __init__(self,   config, \n",
    "                         dataset,\n",
    "                         num_run=0,\n",
    "                         resume_model_path=False,\n",
    "                         resume = False):\n",
    "\n",
    "        \n",
    "        #############################################################################################\n",
    "        # LIBRARIES\n",
    "        #############################################################################################        \n",
    "        import os\n",
    "        import numpy as np\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.python import pywrap_tensorflow\n",
    "        from tensorflow.keras import optimizers, losses, models, backend, layers, metrics\n",
    "        \n",
    "        #self.run_path = os.path.dirname(os.path.realpath(__file__))\n",
    "        #os.chdir(self.run_path)\n",
    "        #core = local_module(\"core\")\n",
    "        #backbones = local_module(\"backbones\")\n",
    "        import core\n",
    "        import backbones\n",
    "        \n",
    "        #############################################################################################\n",
    "        # PARAMETERS RUN\n",
    "        #############################################################################################\n",
    "        self.config          = config\n",
    "        self.dataset         = dataset\n",
    "        self.num_run         = num_run\n",
    "        self.group           = \"All\"\n",
    "        self.name_run        = \"Train_\"+self.group \n",
    "        \n",
    "        self.run_dir         = os.path.join(config[\"PROJECT\"][\"group_dir\"],self.group)\n",
    "        self.run_dir_check   = os.path.join(self.run_dir ,'checkpoints')\n",
    "        self.checkpoints_path= os.path.join(self.run_dir_check,'checkpoint.{epoch:03d}.hdf5')\n",
    "        #self.user            = get_user()\n",
    "        self.stop_flag       = False\n",
    "        self.training_thread = None\n",
    "        self.resume_training = resume\n",
    "        \n",
    "        #self.num_data_train  = len(labeled_set) \n",
    "        self.resume_model_path = resume_model_path\n",
    "        self.transfer_weight_path = self.config['TRAIN'][\"transfer_weight_path\"]\n",
    "        self.input_shape       = [self.config[\"NETWORK\"][\"INPUT_SIZE\"], self.config[\"NETWORK\"][\"INPUT_SIZE\"], 3]\n",
    "        \n",
    "        \n",
    "        self.pre ='\\033[1;36m' + self.name_run + '\\033[0;0m' #\"____\" #\n",
    "        self.problem ='\\033[1;31m' + self.name_run + '\\033[0;0m'\n",
    "        \n",
    "        # Creating the train folde\n",
    "        import shutil\n",
    "        \n",
    "        # create base dir and gr\n",
    "        if os.path.exists(config[\"PROJECT\"][\"project_dir\"]) is False:\n",
    "            os.mkdir(config[\"PROJECT\"][\"project_dir\"])\n",
    "        \n",
    "        if os.path.exists(self.run_dir) and self.resume_model_path is False:\n",
    "            shutil.rmtree(config[\"PROJECT\"][\"group_dir\"])\n",
    "            os.mkdir(config[\"PROJECT\"][\"group_dir\"])\n",
    "            \n",
    "        if os.path.exists(config[\"PROJECT\"][\"group_dir\"]) is False:\n",
    "            os.mkdir(config[\"PROJECT\"][\"group_dir\"])\n",
    "\n",
    "        if os.path.exists(self.run_dir) is False:\n",
    "            os.mkdir(self.run_dir)\n",
    "            \n",
    "        if os.path.exists(self.run_dir_check) is False:\n",
    "            os.mkdir(self.run_dir_check)\n",
    "            \n",
    "        \n",
    "        #############################################################################################\n",
    "        # SETUP TENSORFLOW SESSION\n",
    "        #############################################################################################\n",
    "\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            config_tf = tf.ConfigProto(allow_soft_placement=True) \n",
    "            config_tf.gpu_options.allow_growth = True \n",
    "            self.sess = tf.Session(config=config_tf,graph=self.graph)\n",
    "            with self.sess.as_default():\n",
    "\n",
    "                #############################################################################################\n",
    "                # SETUP WANDB\n",
    "                #############################################################################################\n",
    "                import wandb\n",
    "\n",
    "                self.wandb = wandb\n",
    "                self.wandb.init(project  = config[\"PROJECT\"][\"project\"], \n",
    "                                group    = config[\"PROJECT\"][\"group\"], \n",
    "                                name     = \"Train_\"+str(num_run),\n",
    "                                job_type = self.group ,\n",
    "                                sync_tensorboard = True,\n",
    "                                config = config)\n",
    "\n",
    "                #############################################################################################\n",
    "                # LOAD DATA\n",
    "                #############################################################################################\n",
    "                self.DataGen = core.ClassificationDataset(  config[\"TRAIN\"][\"batch_size\"],\n",
    "                                                             self.dataset,\n",
    "                                                             data_augmentation=config[\"DATASET\"][\"Data_augementation\"],\n",
    "                                                             subset=\"train\")  \n",
    "                \n",
    "                self.num_class = len(self.DataGen.list_classes)\n",
    "                \n",
    "                #############################################################################################\n",
    "                # GLOBAL PROGRESS\n",
    "                #############################################################################################\n",
    "                self.steps_per_epoch  = np.floor(self.DataGen.nb_elements/config[\"TRAIN\"][\"batch_size\"])\n",
    "                self.current_epoch = 0\n",
    "                self.split_epoch   = self.config['TRAIN'][\"EPOCH_WHOLE\"] \n",
    "                self.total_epochs  = self.config['TRAIN'][\"EPOCH_WHOLE\"] + self.config['TRAIN'][\"EPOCH_SLIT\"]\n",
    "                self.progress = round(self.current_epoch / self.total_epochs * 100.0, 2)\n",
    "                \n",
    "                \n",
    "                #############################################################################################\n",
    "                # DEFINE CLASSIFIER\n",
    "                #############################################################################################\n",
    "                # set input\n",
    "                img_input = tf.keras.Input(tensor=self.DataGen.images_tensor,name= 'input_image')\n",
    "\n",
    "                include_top = True\n",
    "\n",
    "                # Get the selected backbone\n",
    "                \"\"\"\n",
    "                ResNet18\n",
    "                ResNet50\n",
    "                ResNet101\n",
    "                ResNet152\n",
    "                ResNet50V2\n",
    "                ResNet101V2\n",
    "                ResNet152V2\n",
    "                ResNeXt50\n",
    "                ResNeXt101\n",
    "                \"\"\"\n",
    "                print(self.pre, \"The backbone is: \",self.config[\"NETWORK\"][\"Backbone\"])\n",
    "                self.backbone = getattr(backbones,self.config[\"NETWORK\"][\"Backbone\"])\n",
    "                #\n",
    "                c_pred_features = self.backbone(input_tensor=img_input, classes= self.num_class, include_top=include_top)\n",
    "                self.c_pred_features=  c_pred_features\n",
    "                if include_top: # include top classifier\n",
    "                    # class predictions\n",
    "                    c_pred = c_pred_features[0]\n",
    "                else:\n",
    "                    x = layers.GlobalAveragePooling2D(name='pool1')(c_pred_features[0])\n",
    "                    x = layers.Dense(self.num_class, name='fc1')(x)\n",
    "                    c_pred = layers.Activation('softmax', name='c_pred')(x)\n",
    "                    c_pred_features[0] = c_pred\n",
    "\n",
    "                self.classifier = models.Model(inputs=[img_input], outputs=c_pred_features,name='Classifier') \n",
    "\n",
    "                #############################################################################################\n",
    "                # DEFINE FULL MODEL\n",
    "                #############################################################################################\n",
    "                c_pred_features_1 = self.classifier(img_input)\n",
    "                c_pred_1 = c_pred_features_1[0]\n",
    "                loss_pred_embeddings = core.Lossnet(c_pred_features_1, self.config[\"NETWORK\"][\"embedding_size\"])\n",
    "                self.model = models.Model(inputs=img_input, outputs=[c_pred_1]+loss_pred_embeddings) #, embedding_s] )\n",
    "                \n",
    "                #############################################################################################\n",
    "                # DEFINE WEIGHT DECAY\n",
    "                #############################################################################################\n",
    "                #core.add_weight_decay(self.model,self.config['TRAIN']['wdecay'])\n",
    "                \n",
    "                #############################################################################################\n",
    "                # DEFINE LOSSES\n",
    "                #############################################################################################\n",
    "                # losses\n",
    "                self.loss_dict = {}\n",
    "                self.loss_dict['Classifier'] = losses.sparse_categorical_crossentropy\n",
    "                self.loss_dict['l_pred_w']   = core.Loss_Lossnet\n",
    "                self.loss_dict['l_pred_s']   = core.Loss_Lossnet\n",
    "                # weights\n",
    "                self.weight_w = backend.variable(self.config['TRAIN']['weight_lossnet_loss'])\n",
    "                self.weight_s = backend.variable(0)\n",
    "\n",
    "                self.loss_w_dict = {}\n",
    "                self.loss_w_dict['Classifier'] = 1\n",
    "                self.loss_w_dict['l_pred_w']   = self.weight_w\n",
    "                self.loss_w_dict['l_pred_s']   = self.weight_s\n",
    "                self.loss_w_dict['Embedding']  = 0\n",
    "\n",
    "                #############################################################################################\n",
    "                # DEFINE METRICS\n",
    "                #############################################################################################\n",
    "                # metrics\n",
    "                self.metrics_dict = {}\n",
    "                self.metrics_dict['Classifier'] = metrics.sparse_categorical_crossentropy\n",
    "                self.metrics_dict['l_pred_w']   = core.MAE_Lossnet\n",
    "                self.metrics_dict['l_pred_s']   = core.MAE_Lossnet\n",
    "\n",
    "                #############################################################################################\n",
    "                # DEFINE OPTIMIZER\n",
    "                #############################################################################################\n",
    "                self.opt = optimizers.Adam(lr=self.config['TRAIN']['lr'])\n",
    "\n",
    "                #############################################################################################\n",
    "                # DEFINE CALLBACKS\n",
    "                #############################################################################################\n",
    "                # Checkpoint saver\n",
    "                self.callbacks = []\n",
    "                model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                        filepath=self.checkpoints_path,\n",
    "                                                        save_weights_only=True,\n",
    "                                                        period=self.config[\"TRAIN\"][\"test_each\"])\n",
    "\n",
    "\n",
    "                self.callbacks.append(model_checkpoint_callback)\n",
    "\n",
    "                # Callback to wandb\n",
    "                self.callbacks.append(self.wandb.keras.WandbCallback())\n",
    "\n",
    "                # Callback Learning Rate\n",
    "                def scheduler(epoch):\n",
    "                    lr = self.config['TRAIN']['lr']\n",
    "                    for i in self.config['TRAIN']['MILESTONES']:\n",
    "                        if epoch>i:\n",
    "                            lr*=0.1\n",
    "                    return lr\n",
    "\n",
    "                self.callbacks.append(tf.keras.callbacks.LearningRateScheduler(scheduler))\n",
    "\n",
    "                # callbeck to change the weigths for the split training:\n",
    "                self.callbacks.append(core.Change_loss_weights(self.weight_w, self.weight_s, self.split_epoch, self.config['TRAIN']['weight_lossnet_loss']))\n",
    "\n",
    "                #############################################################################################\n",
    "                # LOAD PREVIUS WEIGTHS\n",
    "                #############################################################################################\n",
    "                if self.resume_model_path:\n",
    "                    # check the epoch where is loaded\n",
    "                    try:\n",
    "                        loaded_epoch = int(self.resume_model_path.split('.')[-2])\n",
    "                        print(self.pre, \"Loading weigths from: \",self.resume_model_path)\n",
    "                        print(self.pre, \"The detected epoch is: \",loaded_epoch)\n",
    "                        # load weigths\n",
    "                        self.model.load_weights(self.resume_model_path)\n",
    "                    except:\n",
    "                        print( self.problem ,\"=> Problem loading the weights from \",self.resume_model_path)\n",
    "                        print( self.problem ,'=> It will rain from scratch')\n",
    "\n",
    "                if self.resume_training:\n",
    "                    self.current_epoch = loaded_epoch\n",
    "                    self.progress = round(self.current_epoch / self.total_epochs * 100.0, 2)\n",
    "\n",
    "                    if self.current_epoch > self.total_epochs:\n",
    "                        raise ValueError(\"The starting epoch is higher that the total epochs\")\n",
    "                    else:\n",
    "                        print(self.pre, \"Resuming the training from stage: \",self.num_run,\" at epoch \", self.current_epoch)\n",
    "\n",
    "                #############################################################################################\n",
    "                # COMPILE MODEL\n",
    "                #############################################################################################        \n",
    "                self.model.compile(loss = self.loss_dict, \n",
    "                                   loss_weights = self.loss_w_dict, \n",
    "                                   metrics = self.metrics_dict, \n",
    "                                   optimizer = self.opt,\n",
    "                                   target_tensors=self.DataGen.labels_tensor)\n",
    "\n",
    "                #############################################################################################\n",
    "                # INIT VARIABLES\n",
    "                #############################################################################################\n",
    "                #self.sess.graph.as_default()\n",
    "                backend.set_session(self.sess)\n",
    "                self.sess.run(tf.local_variables_initializer())\n",
    "\n",
    "                ################################################################\n",
    "                # SETUP WATCHER\n",
    "                ################################################################    \n",
    "\n",
    "                #self.run_watcher = get_run_watcher()\n",
    "\n",
    "                #self.run_watcher.add_run.remote(name=self.name_run,\n",
    "                #                                user=self.user,\n",
    "                #                                progress=self.progress,\n",
    "                #                                wandb_url=self.wandb.run.get_url(),\n",
    "                #                                status=\"Idle\")\n",
    "                print(self.pre,'Init done')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AutoML import AutoML, DataNet, AutoMLDataset\n",
    "# DataNet().restart_datanodes()\n",
    "dataset = AutoMLDataset(name=\"person_classification\", semantics=[113988, 113989, 113990], crops=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fles = Active_Learning_train(config, \n",
    "                             dataset,\n",
    "                             resume_model_path=False,\n",
    "                             resume = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with fles.graph.as_default():\n",
    "    with fles.sess.as_default():\n",
    "\n",
    "        history = fles.model.fit(  steps_per_epoch = fles.steps_per_epoch,\n",
    "                                   epochs = fles.total_epochs, \n",
    "                                   callbacks = fles.callbacks,\n",
    "                                   initial_epoch = fles.current_epoch,\n",
    "                                   verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
