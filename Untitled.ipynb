{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend, layers, models, utils\n",
    "from tensorflow.keras import datasets, Sequential, preprocessing\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import regularizers\n",
    "from PIL import Image\n",
    "\n",
    "from models.resnet import ResNet18\n",
    "# Inside my model training code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mafospinat\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/opt/conda/lib/python3.6/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.11<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">dainty-water-29</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/afospinat/Active%20Learning\" target=\"_blank\">https://wandb.ai/afospinat/Active%20Learning</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/afospinat/Active%20Learning/runs/o4tiu7ux\" target=\"_blank\">https://wandb.ai/afospinat/Active%20Learning/runs/o4tiu7ux</a><br/>\n",
       "                Run data is saved locally in <code>/workspace/Active Learning/wandb/run-20201123_152352-o4tiu7ux</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from config import wandb\n",
    "from wandb.keras import WandbCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
    "\n",
    "# generate dataloader for train\n",
    "train_datagen = ImageDataGenerator(\n",
    "        width_shift_range=[-2,2],\n",
    "        height_shift_range=[-2,2],\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# generate dataloader for test\n",
    "test_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.input_shape = x_train.shape[1:]\n",
    "wandb.config.classes_data = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"''\\n##\\n# Learning Loss for Active Learning\\nNUM_TRAIN = 50000 # N\\nNUM_VAL   = 50000 - NUM_TRAIN\\nBATCH     = 128 # B\\nSUBSET    = 10000 # M\\nADDENDUM  = 1000 # K\\n\\nMARGIN = 1.0 # xi\\nWEIGHT = 1.0 # lambda\\n\\nTRIALS = 3\\nCYCLES = 10\\n\\nEPOCH = 200\\nLR = 0.1\\nMILESTONES = [160]\\nEPOCHL = 120 # After 120 epochs, stop the gradient from the loss prediction module propagated to the target model\\n\\nMOMENTUM = 0.9\\nWDECAY = 5e-4\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''''\n",
    "##\n",
    "# Learning Loss for Active Learning\n",
    "NUM_TRAIN = 50000 # N\n",
    "NUM_VAL   = 50000 - NUM_TRAIN\n",
    "BATCH     = 128 # B\n",
    "SUBSET    = 10000 # M\n",
    "ADDENDUM  = 1000 # K\n",
    "\n",
    "MARGIN = 1.0 # xi\n",
    "WEIGHT = 1.0 # lambda\n",
    "\n",
    "TRIALS = 3\n",
    "CYCLES = 10\n",
    "\n",
    "EPOCH = 200\n",
    "LR = 0.1\n",
    "MILESTONES = [160]\n",
    "EPOCHL = 120 # After 120 epochs, stop the gradient from the loss prediction module propagated to the target model\n",
    "\n",
    "MOMENTUM = 0.9\n",
    "WDECAY = 5e-4\n",
    "'''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate model\n",
    "def generate_model(config):\n",
    "\n",
    "    def get_embedding_nets():\n",
    "        return Sequential([layers.GlobalAveragePooling2D(),layers.Dense(config.embedding_size),layers.Activation(\"relu\")])\n",
    "\n",
    "    def get_classifcation_net():\n",
    "        return Sequential([layers.GlobalAveragePooling2D(),layers.Dense(config.classes_data),layers.Activation(\"softmax\")])\n",
    "\n",
    "    def get_classifcation_net():\n",
    "        return Sequential([layers.GlobalAveragePooling2D(),layers.Dense(config.classes_data),layers.Activation(\"softmax\")])\n",
    "    \n",
    "    # generate the rest of the model\n",
    "    inputs = tf.keras.Input(shape=config.input_shape)\n",
    "    # add backbone\n",
    "    with tf.variable_scope(\"backbone\"):\n",
    "        backbone = ResNet18(input_shape=config.input_shape, weights='imagenet',include_top=False)\n",
    "        x = backbone(inputs)\n",
    "    # make normal classification\n",
    "    with tf.variable_scope(\"classification\"):\n",
    "        classification = get_classifcation_net()(x[0])\n",
    "        classification = tf.identity(classification,'out_classification')\n",
    "\n",
    "\n",
    "    with tf.variable_scope(\"loss_learning\"):\n",
    "        # generate embeddings for each other output\n",
    "        embeddings_list =[]\n",
    "        for out in x[1:]:\n",
    "            embeddings_list.append(get_embedding_nets()(out))\n",
    "        embedding = layers.Concatenate()(embeddings_list)\n",
    "        embedding = tf.identity(embedding,'out_embedding')\n",
    "        out_loss = layers.Dense(1)(embedding)\n",
    "        out_loss = layers.Concatenate()([classification,out_loss])\n",
    "        out_loss = tf.identity(out_loss,'out_loss_learning')\n",
    "\n",
    "    classifier = models.Model(inputs, [classification,embedding,out_loss])\n",
    "    \n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_loss_v0(c_true, c_pred, l_pred, margin=1.0, reduction='mean'):\n",
    "\n",
    "    assert y_pred.shape[0] % 2 == 0, 'the batch size is not even.'\n",
    "    l_pred_r = l_pred[::-1]\n",
    "    assert l_pred.shape == l_pred_r.shape\n",
    "    \n",
    "    l_pred = (l_pred - l_pred_r)[:y_pred.shape[0]//2]\n",
    "    \n",
    "    # c_true is just the classification as the l_true is calculated here\n",
    "    scc = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')\n",
    "    class_loss = scc(c_true,c_pred)\n",
    "    \n",
    "    target = (class_loss - class_loss[::-1])[:class_loss.shape[0]//2]\n",
    "    target = tf.stop_gradient(target)\n",
    "    \n",
    "    one = (2 * tf.math.sign(  tf.clip_by_value( target, 0, 1))) - 1\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        loss = tf.reduce_sum(tf.clip_by_value(margin - one * l_pred, 0,10000))\n",
    "        loss = loss / tf.cast(l_pred.shape[0], tf.float64)  # Note that the size of l_pred is already halved\n",
    "    elif reduction == 'none':\n",
    "        loss = tf.clip_by_value(margin - one * l_pred, 0,10000)\n",
    "    else:\n",
    "        NotImplementedError()\n",
    "    \n",
    "    return loss,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_leaning_loss(tf.keras.losses.Loss):\n",
    "    def __init__(self, margin=1.0, reduction='mean', name=\"Learning_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.margin=1.0\n",
    "        self.reduction= 'mean'\n",
    "\n",
    "\n",
    "    def call(self, y_true, y_pred):  \n",
    "\n",
    "        c_pred = y_pred[:,:-1]\n",
    "        # loss prediction\n",
    "        l_pred = y_pred[:,-1]\n",
    "        l_pred_r = l_pred[::-1]\n",
    "        #assert tf.shape(y_pred) == tf.shape(l_pred_r)\n",
    "\n",
    "        l_pred = (l_pred - l_pred_r)[:y_pred.shape[0]//2]\n",
    "\n",
    "        # y_true is just the classification as the l_true is calculated here\n",
    "        scc = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')\n",
    "        class_loss = scc(y_true,c_pred)\n",
    "\n",
    "        l_true = (class_loss - class_loss[::-1])[:class_loss.shape[0]//2]\n",
    "        l_true = tf.stop_gradient(l_true)\n",
    "\n",
    "        one = (2 * tf.math.sign(  tf.clip_by_value( l_true, 0, 1))) - 1\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = tf.reduce_sum(tf.clip_by_value(self.margin - one * l_pred, 0,10000))\n",
    "            loss = tf.math.divide(loss , tf.cast(tf.shape(l_pred)[0], loss.dtype) ) # Note that the size of l_pred is already halved\n",
    "        elif self.reduction == 'none':\n",
    "            loss = tf.clip_by_value(self.margin - one * l_pred, 0,10000)\n",
    "        else:\n",
    "            NotImplementedError()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    lr= wandb.config.lr\n",
    "    for i in wandb.config.milestones:\n",
    "        if epoch>i:\n",
    "            lr*=0.1\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "#del (Classification_with_AL)\n",
    "Classification_with_AL = generate_model(wandb.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tf_op_layer_classification/out_classification',\n",
       " 'tf_op_layer_loss_learning/out_embedding',\n",
       " 'tf_op_layer_loss_learning/out_loss_learning']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Classification_with_AL.output_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =[y_train,y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([y_train,y_train]).T[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_gen = test_datagen.flow(x_test,y_test,batch_size=wandb.config.batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict={Classification_with_AL.output_names[0]:tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "           Classification_with_AL.output_names[2]:Loss_leaning_loss()}\n",
    "\n",
    "weigths_dict={ Classification_with_AL.output_names[0]:wandb.config.w_classif_loss,\n",
    "               Classification_with_AL.output_names[1]:0, \n",
    "               Classification_with_AL.output_names[2]:wandb.config.w_loss_loss}\n",
    "\n",
    "metrics={ Classification_with_AL.output_names[0]:tf.keras.metrics.SparseCategoricalAccuracy()}\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD( learning_rate=wandb.config.lr, momentum=wandb.config.momentum)\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "\n",
    "#wandb.config.wdecay = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = test_datagen.flow(x_test,y_test,batch_size=wandb.config.batch_size,shuffle=False)\n",
    "train_gen = train_datagen.flow(x_train,y_train,batch_size=wandb.config.batch_size)\n",
    "\n",
    "\n",
    "def mod_data_gen(generator):\n",
    "    while True:\n",
    "        X,Y = generator.next()\n",
    "        yield X, [Y, Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output tf_op_layer_loss_learning/out_embedding missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to tf_op_layer_loss_learning/out_embedding.\n"
     ]
    }
   ],
   "source": [
    "Classification_with_AL.compile(  optimizer=optimizer,\n",
    "                loss=loss_dict,\n",
    "                loss_weights=weigths_dict,\n",
    "                metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "391/391 [==============================] - 766s 2s/step - loss: 2.0329 - tf_op_layer_classification/out_classification_loss: 2.0329 - tf_op_layer_loss_learning/out_loss_learning_loss: 2.0662 - tf_op_layer_classification/out_classification_sparse_categorical_accuracy: 0.3031\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 757s 2s/step - loss: 1.8643 - tf_op_layer_classification/out_classification_loss: 1.8643 - tf_op_layer_loss_learning/out_loss_learning_loss: 2.8679 - tf_op_layer_classification/out_classification_sparse_categorical_accuracy: 0.3595\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 796s 2s/step - loss: 1.8298 - tf_op_layer_classification/out_classification_loss: 1.8298 - tf_op_layer_loss_learning/out_loss_learning_loss: 2.6117 - tf_op_layer_classification/out_classification_sparse_categorical_accuracy: 0.3732\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 771s 2s/step - loss: 1.8226 - tf_op_layer_classification/out_classification_loss: 1.8226 - tf_op_layer_loss_learning/out_loss_learning_loss: 2.4988 - tf_op_layer_classification/out_classification_sparse_categorical_accuracy: 0.3586\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 777s 2s/step - loss: 1.6927 - tf_op_layer_classification/out_classification_loss: 1.6927 - tf_op_layer_loss_learning/out_loss_learning_loss: 1.5542 - tf_op_layer_classification/out_classification_sparse_categorical_accuracy: 0.4158\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 761s 2s/step - loss: 1.8230 - tf_op_layer_classification/out_classification_loss: 1.8230 - tf_op_layer_loss_learning/out_loss_learning_loss: 2.6807 - tf_op_layer_classification/out_classification_sparse_categorical_accuracy: 0.3773\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 775s 2s/step - loss: 1.8795 - tf_op_layer_classification/out_classification_loss: 1.8795 - tf_op_layer_loss_learning/out_loss_learning_loss: 3.6610 - tf_op_layer_classification/out_classification_sparse_categorical_accuracy: 0.3349\n",
      "Epoch 8/50\n",
      " 33/391 [=>............................] - ETA: 11:55 - loss: 1.6292 - tf_op_layer_classification/out_classification_loss: 1.6292 - tf_op_layer_loss_learning/out_loss_learning_loss: 3.2115 - tf_op_layer_classification/out_classification_sparse_categorical_accuracy: 0.3987"
     ]
    }
   ],
   "source": [
    "Classification_with_AL.fit_generator(mod_data_gen(train_gen), \n",
    "                                     epochs=wandb.config.epoch, \n",
    "                                     steps_per_epoch= len(train_gen), \n",
    "                                     validation_data=mod_data_gen(test_gen),\n",
    "                                     validation_steps=len(test_gen),\n",
    "                                     validation_freq=10,\n",
    "                                     callbacks=[WandbCallback(),callback])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
