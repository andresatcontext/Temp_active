{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, name = \"X\")\n",
    "label = tf.placeholder(tf.float32, name = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(l_pred_w,class_loss_non_reducted, reduction='mean', margin=1.0):\n",
    "\n",
    "    # compute the classification loss non reducted\n",
    "    get_batch_size = tf.shape(class_loss_non_reducted)[0]\n",
    "\n",
    "\n",
    "    with tf.compat.v1.variable_scope(\"Reference_Loss_LossNet\"):\n",
    "        l_true = (class_loss_non_reducted - class_loss_non_reducted[::-1] )[:class_loss_non_reducted.shape[0]//2]\n",
    "        # get the value without the gradient\n",
    "        l_true = tf.stop_gradient(l_true)\n",
    "\n",
    "\n",
    "    # value used in the lossnet loss\n",
    "    one = (2 * tf.math.sign(  tf.clip_by_value( l_true, 0, 1))) - 1\n",
    "\n",
    "\n",
    "    with tf.compat.v1.variable_scope(\"Learning_loss_loss_whole\"):\n",
    "        if reduction == 'mean':\n",
    "            l_loss_w = tf.reduce_sum(tf.clip_by_value(margin - one * l_pred_w, 0,10000))\n",
    "            l_loss_w = tf.math.divide(l_loss_w , tf.cast(tf.shape(l_pred_w)[0], l_loss_w.dtype) ) # Note that the size of l_pred is already halved\n",
    "        elif reduction == 'none':\n",
    "            l_loss_w = tf.clip_by_value(margin - one * l_pred_w, 0,10000)\n",
    "        else:\n",
    "            NotImplementedError()\n",
    "\n",
    "    with tf.compat.v1.variable_scope(\"Learning_loss_loss_split\"):\n",
    "        if reduction == 'mean':\n",
    "            l_loss_s = tf.reduce_sum(tf.clip_by_value(margin - one * l_pred_w, 0,10000))\n",
    "            l_loss_s = tf.math.divide(l_loss_s , tf.cast(tf.shape(l_pred_w)[0], l_loss_s.dtype) ) # Note that the size of l_pred is already halved\n",
    "        elif reduction == 'none':\n",
    "            l_loss_s = tf.clip_by_value(margin - one * l_pred_w, 0,10000)\n",
    "        else:\n",
    "            NotImplementedError()\n",
    "\n",
    "    return l_loss_w, l_loss_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossPredLoss(x, target, margin=1.0, reduction='mean'):\n",
    "    assert len(x) % 2 == 0, 'the batch size is not even.'\n",
    "    assert x.shape == x.flip(0).shape\n",
    "    \n",
    "    x = (x - x.flip(0))[:len(x)//2] # [l_1 - l_2B, l_2 - l_2B-1, ... , l_B - l_B+1], where batch_size = 2B\n",
    "    target = (target - target.flip(0))[:len(target)//2]\n",
    "    target = target.detach()\n",
    "\n",
    "    one = 2 * torch.sign(torch.clamp(target, min=0)) - 1 # 1 operation which is defined by the authors\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        loss = torch.sum(torch.clamp(margin - one * x, min=0))\n",
    "        loss = loss / x.size(0) # Note that the size of x is already halved\n",
    "    elif reduction == 'none':\n",
    "        loss = torch.clamp(margin - one * x, min=0)\n",
    "    else:\n",
    "        NotImplementedError()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(x,target, reduction='mean', margin=1.0):\n",
    "\n",
    "    # compute the classification loss non reducted\n",
    "    get_batch_size = tf.shape(target)[0]\n",
    "    \n",
    "    \n",
    "    x = (x - x[::-1])[:get_batch_size//2]\n",
    "    target = (target - target[::-1] )[:get_batch_size//2]\n",
    "    target = tf.stop_gradient(target)\n",
    "\n",
    "    # value used in the lossnet loss\n",
    "    one = (2 * tf.math.sign(tf.clip_by_value( target, 0, 1))) - 1\n",
    "\n",
    "    if reduction == 'mean':\n",
    "        loss = tf.reduce_sum(tf.clip_by_value(margin - one * x, 0,10000))\n",
    "        loss = tf.math.divide(loss , tf.cast(get_batch_size//2, loss.dtype) ) # Note that the size of l_pred is already halved\n",
    "    elif reduction == 'none':\n",
    "        loss = tf.clip_by_value(margin - one * x, 0,10000)\n",
    "    else:\n",
    "        NotImplementedError()\n",
    "\n",
    "\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(self,c_true):\n",
    "\n",
    "    # compute the classification loss non reducted\n",
    "    get_batch_size = tf.shape(c_true)[0]\n",
    "\n",
    "    # Classification loss\n",
    "    with tf.compat.v1.variable_scope(\"Classification_loss\"):\n",
    "        class_loss_non_reducted = tf.nn.softmax_cross_entropy_with_logits_v2(labels=c_true, logits=self.c_pred)\n",
    "        c_loss = tf.reduce_mean(class_loss_non_reducted)\n",
    "\n",
    "    with tf.compat.v1.variable_scope(\"Reference_Loss_LossNet\"):\n",
    "        l_true = (class_loss_non_reducted - class_loss_non_reducted[::-1] )[:get_batch_size//2]\n",
    "        # get the value without the gradient\n",
    "        l_true = tf.stop_gradient(l_true)\n",
    "        \n",
    "    \n",
    "    # value used in the lossnet loss\n",
    "    one = (2 * tf.math.sign(  tf.clip_by_value( l_true, 0, 1))) - 1\n",
    "\n",
    "    with tf.compat.v1.variable_scope(\"Learning_loss_loss_whole\"):\n",
    "        \n",
    "        l_pred_w = (self.l_pred_w - self.l_pred_w[::-1])[:get_batch_size//2]\n",
    "        if self.reduction == 'mean':\n",
    "            l_loss_w = tf.reduce_sum(tf.clip_by_value(self.margin - one * l_pred_w, 0,10000))\n",
    "            l_loss_w = tf.math.divide(l_loss_w , tf.cast(tf.shape(l_pred_w)[0], l_loss_w.dtype) ) # Note that the size of l_pred is already halved\n",
    "        elif self.reduction == 'none':\n",
    "            l_loss_w = tf.clip_by_value(self.margin - one * self.l_pred_w, 0,10000)\n",
    "        else:\n",
    "            NotImplementedError()\n",
    "\n",
    "    with tf.compat.v1.variable_scope(\"Learning_loss_loss_split\"):\n",
    "        l_pred_s = (self.l_pred_s - self.l_pred_s[::-1])[:get_batch_size//2]\n",
    "        if self.reduction == 'mean':\n",
    "            l_loss_s = tf.reduce_sum(tf.clip_by_value(self.margin - one * l_pred_s, 0,10000))\n",
    "            l_loss_s = tf.math.divide(l_loss_s , tf.cast(tf.shape(self.l_pred_s)[0], l_loss_s.dtype) ) # Note that the size of l_pred is already halved\n",
    "        elif self.reduction == 'none':\n",
    "            l_loss_s = tf.clip_by_value(self.margin - one * self.l_pred_s, 0,10000)\n",
    "        else:\n",
    "            NotImplementedError()\n",
    "\n",
    "    self.c_loss =  c_loss\n",
    "    self.l_loss_w = l_loss_w\n",
    "    self.l_loss_s = l_loss_s\n",
    "    self.l_true  = l_true\n",
    "    self.class_loss_non_reducted = class_loss_non_reducted\n",
    "\n",
    "    return self.c_loss, self.l_loss_w, self.l_loss_s, self.l_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = compute_loss(X,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_tf = tf.ConfigProto(allow_soft_placement=True) \n",
    "config_tf.gpu_options.allow_growth = True \n",
    "sess = tf.Session(config=config_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([0.5023, 0.5100, 0.9195, 0.9535, 0.6252, 0.8706, 0.6801, 0.4186, 0.9509,0.3584, 0.5273, 0.9775, 0.4921, 0.4515, 0.7213, 0.8194])\n",
    "target= np.array([0.0736, 0.0751, 0.8538, 0.7110, 0.9743, 0.7315, 0.1218, 0.6964, 0.4264, 0.4585, 0.3606, 0.0911, 0.9101, 0.0578, 0.1127, 0.6333])\n",
    "\n",
    "\n",
    "x = np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "target= np.array([10,9,8,7,6,5,4,3,2,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "result = sess.run(res, feed_dict={X:x, label:target})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.21069985, 0.90130608, 0.31085171, 0.83086515],\n",
       "       [0.37410988, 0.70285848, 0.83830728, 0.55840195]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.random.rand(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80238206, 0.70935415, 0.99898735, 0.53499073, 0.42875013,\n",
       "       0.46676256, 0.02110695, 0.77252365, 0.52069968, 0.47556882,\n",
       "       0.26257111, 0.47124933, 0.803462  , 0.11010067, 0.98035664,\n",
       "       0.21934148])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21934148, 0.98035664, 0.11010067, 0.803462  , 0.47124933,\n",
       "       0.26257111, 0.47556882, 0.52069968, 0.77252365, 0.02110695,\n",
       "       0.46676256, 0.42875013, 0.53499073, 0.99898735, 0.70935415,\n",
       "       0.80238206])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
