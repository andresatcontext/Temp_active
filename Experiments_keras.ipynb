{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load defaulft config\n",
    "import yaml\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "config_path = './configs/default_cifar.yml'\n",
    "\n",
    "with open(config_path) as file:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    \n",
    "\n",
    "# create base dir and gr\n",
    "if os.path.exists(config[\"PROJECT\"][\"project_dir\"]) is False:\n",
    "    os.mkdir(config[\"PROJECT\"][\"project_dir\"])\n",
    "\n",
    "if os.path.exists(config[\"PROJECT\"][\"group_dir\"]) is False:\n",
    "    os.mkdir(config[\"PROJECT\"][\"group_dir\"])\n",
    "    \n",
    "    \n",
    "# Get the data to annotate\n",
    "\n",
    "#############################################################################################\n",
    "# LOAD DATA\n",
    "#############################################################################################\n",
    "from data_utils import CIFAR10Data\n",
    "# Load data\n",
    "cifar10_data = CIFAR10Data()\n",
    "num_classes = len(cifar10_data.classes)\n",
    "x_train, y_train, x_test, y_test = cifar10_data.get_data(normalize_data=False)\n",
    "\n",
    "indices = list(range(len(x_train)))\n",
    "random.seed(101)\n",
    "random.shuffle(indices)\n",
    "labeled_set = indices\n",
    "unlabeled_set =[]\n",
    "\n",
    "\n",
    "# test with all the images\n",
    "NUM_IMAGES_TEST = len(x_test)\n",
    "# Initialize a labeled dataset by randomly sampling K=ADDENDUM=1,000 data points from the entire dataset.\n",
    "test_set = list(range(NUM_IMAGES_TEST))\n",
    "\n",
    "config[\"NETWORK\"][\"INPUT_SIZE\"] =  x_train[0].shape[0]\n",
    "config[\"NETWORK\"][\"CLASSES\"] = cifar10_data.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACTIVE_ALGO': {'LOSSLEARNING': 1.0},\n",
       " 'DATASET': {'height_shift_range': 4,\n",
       "  'horizontal_flip': True,\n",
       "  'width_shift_range': 4},\n",
       " 'NETWORK': {'CLASSES': ['plane',\n",
       "   'car',\n",
       "   'bird',\n",
       "   'cat',\n",
       "   'deer',\n",
       "   'dog',\n",
       "   'frog',\n",
       "   'horse',\n",
       "   'ship',\n",
       "   'truck'],\n",
       "  'INPUT_SIZE': 32,\n",
       "  'MARGIN': 1.0,\n",
       "  'embedding_size': 128},\n",
       " 'PROJECT': {'Backbone': 'resnet18',\n",
       "  'dataset_name': 'CIFAR',\n",
       "  'group': 'Classif_all_data_0912',\n",
       "  'group_dir': '/mnt/Ressources/Andres/Temp_active/runs/Classif_all_data_0912',\n",
       "  'project': 'Active_Learning_CIFAR',\n",
       "  'project_dir': '/mnt/Ressources/Andres/Temp_active/runs',\n",
       "  'source': 'CIFAR'},\n",
       " 'RUNS': {'ADDENDUM': 1000,\n",
       "  'CYCLES': 1,\n",
       "  'SUBSET': -1,\n",
       "  'TRIALS': 1,\n",
       "  'test_each': 1},\n",
       " 'TEST': {'batch_size': 128},\n",
       " 'TRAIN': {'Data_augementation': True,\n",
       "  'EPOCH_SLIT': 20,\n",
       "  'EPOCH_WARMUP': 2,\n",
       "  'EPOCH_WHOLE': 40,\n",
       "  'MILESTONES': [25, 35],\n",
       "  'batch_size': 128,\n",
       "  'gamma': 0.1,\n",
       "  'lr': 0.01,\n",
       "  'start_epoch': 0,\n",
       "  'transfer_weight_path': False,\n",
       "  'w_c_loss': 1.0,\n",
       "  'w_l_loss': 0,\n",
       "  'wdecay': 0.995}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Active_Learning_train:\n",
    "    def __init__(self,   config, \n",
    "                         labeled_set,\n",
    "                         test_set, \n",
    "                         num_run,\n",
    "                         resume_model_path):\n",
    "\n",
    "        \n",
    "        #############################################################################################\n",
    "        # LIBRARIES\n",
    "        #############################################################################################        \n",
    "        import os\n",
    "        #self.run_path = os.path.dirname(os.path.realpath(__file__))\n",
    "        #os.chdir(self.run_path)\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        import core\n",
    "        #core = local_module(\"core\")\n",
    "        import backbones\n",
    "        #backbones = local_module(\"backbones\")\n",
    "        \n",
    "        from tensorflow.python import pywrap_tensorflow\n",
    "        import numpy as np\n",
    "        from tensorflow.keras import optimizers, losses, models, backend, layers, metrics\n",
    "        \n",
    "        #############################################################################################\n",
    "        # SETUP TENSORFLOw SESSION\n",
    "        #############################################################################################\n",
    "        config_tf = tf.ConfigProto(allow_soft_placement=True) \n",
    "        config_tf.gpu_options.allow_growth = True \n",
    "        self.sess = tf.Session(config=config_tf)\n",
    "        self.sess.graph.as_default()\n",
    "        self.K_sess = backend.set_session(self.sess)\n",
    "    \n",
    "        \n",
    "        #############################################################################################\n",
    "        # PARAMETERS RUN\n",
    "        #############################################################################################\n",
    "        self.config          = config\n",
    "        self.num_run         = num_run\n",
    "        self.group           = \"Stage_\"+str(num_run)\n",
    "        self.name_run        = \"Train_\"+self.group \n",
    "        \n",
    "        self.run_dir         = os.path.join(config[\"PROJECT\"][\"group_dir\"],self.group)\n",
    "        self.run_dir_check   = os.path.join(self.run_dir ,'checkpoints')\n",
    "        self.checkpoints_path= os.path.join(self.run_dir_check,'checkpoint.{epoch:03d}.hdf5')\n",
    "        #self.user            = get_user()\n",
    "        self.test_run_id     = None\n",
    "        self.stop_flag       = False\n",
    "        self.training_thread = None\n",
    "        \n",
    "        self.num_data_train  = len(labeled_set) \n",
    "        self.resume_model_path = resume_model_path\n",
    "        self.transfer_weight_path = self.config['TRAIN'][\"transfer_weight_path\"]\n",
    "        self.num_class       = len(self.config[\"NETWORK\"][\"CLASSES\"])\n",
    "        self.input_shape     = [self.config[\"NETWORK\"][\"INPUT_SIZE\"], self.config[\"NETWORK\"][\"INPUT_SIZE\"], 3]\n",
    "        \n",
    "        \n",
    "        self.pre ='\\033[1;36m' + self.name_run + '\\033[0;0m' #\"____\" #\n",
    "        self.problem ='\\033[1;31m' + self.name_run + '\\033[0;0m'\n",
    "        \n",
    "        # Creating the train folde\n",
    "        import shutil\n",
    "        \n",
    "        if os.path.exists(self.run_dir) and self.resume_model_path is False:\n",
    "            if num_run==0:\n",
    "                shutil.rmtree(config[\"PROJECT\"][\"group_dir\"])\n",
    "                os.mkdir(config[\"PROJECT\"][\"group_dir\"])\n",
    "            else:  \n",
    "                shutil.rmtree(self.run_dir)\n",
    "                \n",
    "        if os.path.exists(self.run_dir) is False:\n",
    "            os.mkdir(self.run_dir)\n",
    "            \n",
    "        if os.path.exists(self.run_dir_check) is False:\n",
    "            os.mkdir(self.run_dir_check)\n",
    "\n",
    "            \n",
    "        #############################################################################################\n",
    "        # SETUP WANDB\n",
    "        #############################################################################################\n",
    "        \"\"\"\n",
    "        import wandb\n",
    "        from wandb.keras import WandbCallback\n",
    "        \n",
    "        self.wandb = wandb\n",
    "        self.wandb.init(project  = config[\"PROJECT\"][\"project\"], \n",
    "                        group    = config[\"PROJECT\"][\"group\"], \n",
    "                        name     = \"Train_\"+str(num_run),\n",
    "                        job_type = self.group ,\n",
    "                        sync_tensorboard = True,\n",
    "                        config = config)\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #############################################################################################\n",
    "        # GLOBAL PROGRESS\n",
    "        #############################################################################################\n",
    "        self.start_epoch   = 0\n",
    "        self.current_epoch = 0\n",
    "        self.split_epoch   = self.config['TRAIN'][\"EPOCH_WHOLE\"] \n",
    "        self.total_epochs  = self.config['TRAIN'][\"EPOCH_WHOLE\"] + self.config['TRAIN'][\"EPOCH_SLIT\"]\n",
    "        self.progress = round(self.current_epoch / self.total_epochs * 100.0, 2)\n",
    "\n",
    "        #############################################################################################\n",
    "        # LOAD DATA\n",
    "        #############################################################################################\n",
    "        if self.config[\"PROJECT\"][\"source\"]=='CIFAR':\n",
    "            from data_utils import CIFAR10Data\n",
    "            # Load data\n",
    "            cifar10_data = CIFAR10Data()\n",
    "            x_train, y_train, _, _ = cifar10_data.get_data(normalize_data=False)\n",
    "\n",
    "            x_train = x_train[labeled_set]\n",
    "            y_train = y_train[labeled_set]\n",
    "            \n",
    "            self.test_set = test_set\n",
    "        else:\n",
    "            raise NameError('This is not implemented yet')\n",
    "        \n",
    "        \n",
    "        #############################################################################################\n",
    "        # DATA GENERATOR\n",
    "        #############################################################################################\n",
    "        self.Data_Generator = core.Generator_cifar_train(x_train, y_train, config)\n",
    "\n",
    "\n",
    "        #############################################################################################\n",
    "        # GENERATE MODEL\n",
    "        #############################################################################################\n",
    "\n",
    "        \"\"\"\n",
    "        ResNet18\n",
    "        ResNet50\n",
    "        ResNet101\n",
    "        ResNet152\n",
    "        ResNet50V2\n",
    "        ResNet101V2\n",
    "        ResNet152V2\n",
    "        ResNeXt50\n",
    "        ResNeXt101\n",
    "        \"\"\"\n",
    "        #############################################################################################\n",
    "        # DEFINE CLASSIFIER\n",
    "        #############################################################################################\n",
    "        # set input\n",
    "        img_input = tf.keras.Input(self.input_shape,name= 'input_image')\n",
    "\n",
    "        include_top = True\n",
    "\n",
    "        # Get the selected backbone\n",
    "        self.backbone = getattr(backbones,\"ResNet18_cifar\")\n",
    "        #\n",
    "        c_pred_features = self.backbone(input_tensor=img_input, classes= self.num_class, include_top=include_top)\n",
    "        self.c_pred_features= c_pred_features\n",
    "        if include_top: # include top classifier\n",
    "            # class predictions\n",
    "            c_pred = c_pred_features[0]\n",
    "        else:\n",
    "            x = layers.GlobalAveragePooling2D(name='pool1')(c_pred_features[0])\n",
    "            x = layers.Dense(self.num_class, name='fc1')(x)\n",
    "            c_pred = layers.Activation('softmax', name='c_pred')(x)\n",
    "            c_pred_features[0]=c_pred\n",
    "\n",
    "        self.classifier = models.Model(inputs=[img_input], outputs=c_pred_features,name='Classifier') \n",
    "\n",
    "        #############################################################################################\n",
    "        # DEFINE FULL MODEL\n",
    "        #############################################################################################\n",
    "        c_pred_features_1 = self.classifier(img_input)\n",
    "        c_pred_1 = c_pred_features_1[0]\n",
    "\n",
    "        # define lossnet\n",
    "        loss_pred_embeddings = core.Lossnet(c_pred_features_1, self.config[\"NETWORK\"][\"embedding_size\"])\n",
    "\n",
    "        self.model = models.Model(inputs=img_input, outputs=[c_pred_1]+loss_pred_embeddings) #, embedding_s] )\n",
    "        \n",
    "        #############################################################################################\n",
    "        # DEFINE LOSSES\n",
    "        #############################################################################################\n",
    "        # losses\n",
    "        self.loss_dict = {}\n",
    "        self.loss_dict['Classifier'] = losses.categorical_crossentropy\n",
    "        self.loss_dict['l_pred_w']   = core.Simple_loss_mean\n",
    "        self.loss_dict['l_pred_s']   = core.Simple_loss_mean\n",
    "        # wegiths\n",
    "        self.loss_w_dict = {}\n",
    "        self.loss_w_dict['Classifier'] = 1\n",
    "        self.loss_w_dict['l_pred_w']   = 0\n",
    "        self.loss_w_dict['l_pred_s']   = 0\n",
    "        self.loss_w_dict['Embedding']  = 0\n",
    "        \n",
    "        #############################################################################################\n",
    "        # DEFINE METRICS\n",
    "        #############################################################################################\n",
    "        # metrics\n",
    "        self.metrics_dict = {}\n",
    "        self.metrics_dict['Classifier'] = metrics.categorical_accuracy\n",
    "        self.metrics_dict['l_pred_w']   = core.Simple_metric_mean\n",
    "        self.metrics_dict['l_pred_s']   = core.Simple_metric_mean\n",
    "        \n",
    "        #############################################################################################\n",
    "        # DEFINE OPTIMIZER\n",
    "        #############################################################################################\n",
    "        self.opt = optimizers.Adam(lr=0.01)\n",
    "        \n",
    "        #############################################################################################\n",
    "        # DEFINE CALLBACKS\n",
    "        #############################################################################################\n",
    "        # Checkpoint saver\n",
    "        self.callbacks = []\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                filepath=self.checkpoints_path,\n",
    "                                                save_weights_only=True,\n",
    "                                                period=self.config[\"RUNS\"][\"test_each\"])\n",
    "        \n",
    "        \n",
    "        self.callbacks.append(model_checkpoint_callback)\n",
    "        \n",
    "        # Callback to wandb\n",
    "        #self.callbacks.append(WandbCallback())\n",
    "        \n",
    "        # Callback Learning Rate\n",
    "        def scheduler(epoch):\n",
    "            lr = self.config['TRAIN']['lr']\n",
    "            for i in self.config['TRAIN']['MILESTONES']:\n",
    "                if epoch>i:\n",
    "                    lr*=0.1\n",
    "            return lr\n",
    "        \n",
    "        self.callbacks.append(tf.keras.callbacks.LearningRateScheduler(scheduler))\n",
    "        \n",
    "        \n",
    "        #############################################################################################\n",
    "        # LOAD PREVIUS WEIGTHS\n",
    "        #############################################################################################\n",
    "        if self.resume_model_path:\n",
    "            # check the epoch where is loaded\n",
    "            try:\n",
    "                current_epoch = int(self.resume_model_path.split('.')[-2])\n",
    "                print(self.pre, \"Loading weigths from: \",self.resume_model_path)\n",
    "                print(self.pre, \"The detected epoch is: \",current_epoch)\n",
    "                # load weigths\n",
    "                self.model.load_weights(self.resume_model_path)\n",
    "                #\n",
    "                self.start_epoch   = current_epoch\n",
    "                self.current_epoch = current_epoch\n",
    "            except:\n",
    "                print( self.problem ,\"=> Problem loading the weights from \",self.resume_model_path)\n",
    "                print( self.problem ,'=> It will rain from scratch')\n",
    "\n",
    "        #############################################################################################\n",
    "        # COMPILE MODEL\n",
    "        #############################################################################################        \n",
    "        self.model.compile(loss = self.loss_dict, \n",
    "                           loss_weights = self.loss_w_dict, \n",
    "                           metrics = self.metrics_dict, \n",
    "                           optimizer = self.opt\n",
    "                          )\n",
    "    \n",
    "        #############################################################################################\n",
    "        # INIT VARIABLES\n",
    "        #############################################################################################\n",
    "        \n",
    "        self.sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers, losses, models, backend, layers, metrics\n",
    "import tensorflow as tf\n",
    "import core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\u001b[1;36mTrain_Stage_5000\u001b[0;0m Loading weigths from:  /mnt/Ressources/Andres/Temp_active/runs/Classif_all_data_0912/Stage_5000/checkpoints/checkpoint.002.hdf5\n",
      "\u001b[1;36mTrain_Stage_5000\u001b[0;0m The detected epoch is:  2\n",
      "WARNING:tensorflow:Output \"Embedding\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"Embedding\".\n",
      "WARNING:tensorflow:Output \"Embedding\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"Embedding\".\n",
      "WARNING:tensorflow:From /home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py:363: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "resume_model_path = '/mnt/Ressources/Andres/Temp_active/runs/Classif_all_data_0912/Stage_5000/checkpoints/checkpoint.002.hdf5'\n",
    "\n",
    "fles = Active_Learning_train(config,labeled_set,[],5000,resume_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jistory = fles.model.fit_generator(fles.Data_Generator,\n",
    "                                   epochs=4, \n",
    "                                   callbacks = fles.callbacks,\n",
    "                                   initial_epoch=fles.start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jistory.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
