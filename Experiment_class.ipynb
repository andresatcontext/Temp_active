{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#RUN_AT_FATHER = False\n",
    "\n",
    "#from AutoML import DataNet, AutoMLDataset, local_module, local_file, AutoML, get_run_watcher, get_user\n",
    "#import ray\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend, layers, models, utils, losses, regularizers\n",
    "from tensorflow.keras import datasets, Sequential, preprocessing\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "#gpu = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(gpu[0], True)\n",
    "'''\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "'''\n",
    "from tqdm import tqdm\n",
    "\n",
    "class AL_train_cifar():\n",
    "\n",
    "    def __init__(self,config, labeled_set, project=\"AL_CIFAR\" , source=None, dataset_name=None, val_source=None, val_dataset_name=None, bs=32, agnostic_eval=False):\n",
    "        \n",
    "        import os\n",
    "        from core.Classifier_AL import Classifier_AL\n",
    "        from backbones.resnet18_paper import ResNet18\n",
    "        from tensorflow.python import pywrap_tensorflow\n",
    "        import numpy as np\n",
    "        \n",
    "        self.backbone = ResNet18\n",
    "        \n",
    "        \n",
    "        #############################################################################################\n",
    "        # PARAMETERS RUN\n",
    "        #############################################################################################\n",
    "        self.project = project\n",
    "        self.runs_folder = \"./runs\"\n",
    "        self.run_dir, self.name_run = self.get_run_dir()\n",
    "        self.group                  = \"TEST_AL\"\n",
    "        self.user                   = \"Andres\"\n",
    "        \n",
    "        self.initial_weight =False\n",
    "        \n",
    "        # Creating the Run folder\n",
    "        import shutil\n",
    "        if os.path.exists(self.run_dir) and self.initial_weight is False:\n",
    "            shutil.rmtree(self.run_dir)\n",
    "        if os.path.exists(self.run_dir) is False:\n",
    "            os.mkdir(self.run_dir)\n",
    "        \n",
    "        \n",
    "        #############################################################################################\n",
    "        # SETUP TENSORFLOE SESSION\n",
    "        #############################################################################################\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "        \n",
    "        \n",
    "        #############################################################################################\n",
    "        # LOAD DATA\n",
    "        #############################################################################################\n",
    "        from data_utils import CIFAR10Data\n",
    "        # Load data\n",
    "        cifar10_data = CIFAR10Data()\n",
    "        num_classes = len(cifar10_data.classes)\n",
    "        x_train, y_train, x_test, y_test = cifar10_data.get_data(subtract_mean=True)\n",
    "        \n",
    "        x_train = x_train[labeled_set]\n",
    "        y_train = y_train[labeled_set]\n",
    "        \n",
    "        \n",
    "        #############################################################################################\n",
    "        # SETUP WANDB\n",
    "        #############################################################################################\n",
    "        import wandb\n",
    "        self.wandb = wandb\n",
    "        self.wandb.init(project=self.project, group=self.group, job_type=\"train\", sync_tensorboard=True,config=config)\n",
    "        self.config = self.wandb.config\n",
    "        \n",
    "        \n",
    "        #############################################################################################\n",
    "        # GENERATE MODEL\n",
    "        #############################################################################################\n",
    "        train_datagen = ImageDataGenerator(\n",
    "                    width_shift_range=self.config.width_shift_range,\n",
    "                    height_shift_range=self.config.height_shift_range,\n",
    "                    horizontal_flip=self.config.horizontal_flip)\n",
    "\n",
    "        train_gen = train_datagen.flow(x_train,\n",
    "                                       y_train,\n",
    "                                       batch_size=self.config.batch_size)\n",
    "        \n",
    "        features_shape = [None, 32, 32, 3]\n",
    "        labels_shape = [None, 10]\n",
    "\n",
    "        tf_data = tf.data.Dataset.from_generator(lambda: train_gen, \n",
    "                                                 output_types=(tf.float32, tf.float32),\n",
    "                                                 output_shapes = (tf.TensorShape(features_shape), tf.TensorShape(labels_shape)))\n",
    "        tf_data = tf_data.repeat()\n",
    "        #tf_data = tf_data.batch(self.config.batch_size)\n",
    "        #tf_data = tf_data.prefetch(100)\n",
    "        data_tensors = tf_data.make_one_shot_iterator().get_next()\n",
    "        self.img_input = data_tensors[0]\n",
    "        self.c_true    = data_tensors[1]\n",
    "        \n",
    "        #############################################################################################\n",
    "        # GENERATE MODEL\n",
    "        #############################################################################################\n",
    "        with tf.name_scope(\"define_loss\"):\n",
    "            # define inputs to test \n",
    "            #self.img_input = layers.Input(shape=self.config.input_shape, name=\"img_input\")\n",
    "            #c_true = layers.Input(shape=(self.config.num_class), name=\"c_true\")\n",
    "            \n",
    "            self.trainable = tf.compat.v1.placeholder(dtype=tf.bool, name='training')\n",
    "            # get the classifier\n",
    "            self.model = Classifier_AL(self.backbone, self.img_input, self.trainable, self.config)\n",
    "            # get global variables\n",
    "            self.net_var = tf.compat.v1.global_variables()\n",
    "\n",
    "            self.Classification_loss, self.Learning_loss_loss_whole , self.Learning_loss_loss_split, self.l_true = self.model.compute_loss(self.c_true)\n",
    "            \n",
    "            self.loss_split =  (self.config.w_c_loss*self.Classification_loss) + (self.config.w_l_loss * self.Learning_loss_loss_whole)\n",
    "            self.loss_whole =  (self.config.w_c_loss*self.Classification_loss) + (self.config.w_l_loss * self.Learning_loss_loss_split)\n",
    "            \n",
    "            \n",
    "        #############################################################################################\n",
    "        # LOAD PREVIUS TRAINED MODEL\n",
    "        #############################################################################################\n",
    "        #TODO this should load a checkpoint of a training based in the training epoch or\n",
    "        # this could load any saved model for transfer learning\n",
    "        self.base_weight_path = False\n",
    "        with tf.name_scope('loader_and_saver'):\n",
    "            if self.base_weight_path is not False and self.start_epoch == 0:\n",
    "                try:\n",
    "                    reader = pywrap_tensorflow.NewCheckpointReader(self.base_weight_path)\n",
    "                    ckpt_var = reader.get_variable_to_shape_map()\n",
    "\n",
    "                    var_to_restore = []\n",
    "                    for v in self.net_var:\n",
    "                        dict_name = v.name.split(':')[0]\n",
    "                        if dict_name in ckpt_var:\n",
    "                            if tuple(ckpt_var[dict_name]) == v.shape:\n",
    "                                print('Varibles restored: %s' % v.name)\n",
    "                                var_to_restore.append(v)\n",
    "                except Exception as e:\n",
    "                    print(str(e))\n",
    "                    var_to_restore = self.net_var\n",
    "\n",
    "                self.loader = tf.compat.v1.train.Saver(var_to_restore)\n",
    "            else:\n",
    "                self.loader = tf.compat.v1.train.Saver(self.net_var)\n",
    "            self.saver  = tf.compat.v1.train.Saver(tf.global_variables(), max_to_keep=40)\n",
    "        \n",
    "            \n",
    "        #############################################################################################\n",
    "        # GLOBAL PROGRESS\n",
    "        #############################################################################################\n",
    "        self.start_epoch  = self.config.start_epoch\n",
    "        self.total_epochs = self.config.EPOCH_WHOLE +self.config.EPOCH_SLIT \n",
    "        self.steps_per_epoch = int(self.config.NUM_TRAIN / self.config.batch_size)\n",
    "        self.global_step_current = self.steps_per_epoch * self.start_epoch\n",
    "        self.global_step_goal = self.steps_per_epoch * self.total_epochs\n",
    "        self.progress = round(self.global_step_current / self.global_step_goal * 100.0, 2)\n",
    "        \n",
    "        \n",
    "        #############################################################################################\n",
    "        # DEFINE LEARNING RATE FOR TRAINING\n",
    "        #############################################################################################\n",
    "        with tf.name_scope('learning_rate'):\n",
    "            # define the current step as tf variable\n",
    "            self.global_step = tf.Variable(float(self.global_step_current), dtype=tf.float64, trainable=False, name='global_step')\n",
    "            \n",
    "            # define warmup_steps in tf\n",
    "            warmup_steps = tf.constant(self.config.EPOCH_WARMUP * self.steps_per_epoch, dtype=tf.float64, name='warmup_steps')\n",
    "            # define train_steps in tf\n",
    "            train_steps = tf.constant((self.config.EPOCH_WHOLE +self.config.EPOCH_SLIT )*self.steps_per_epoch, dtype=tf.float64, name='train_steps')\n",
    "            \n",
    "            steps_per_epoch = tf.constant(self.steps_per_epoch, dtype=tf.float64, name='steps_per_epoch')\n",
    "            MILESTONES = tf.constant(self.config.MILESTONES, dtype=tf.float64)\n",
    "            \n",
    "            self.learning_rate = tf.cond(   pred = self.global_step < warmup_steps,\n",
    "                                            true_fn=lambda: self.global_step / warmup_steps * self.config.lr,\n",
    "                                            false_fn=lambda: self.config.gamma ** (tf.compat.v1.reduce_sum(tf.cast(MILESTONES<(self.global_step/steps_per_epoch), tf.float64))))\n",
    "            \n",
    "            global_step_update = tf.assign_add(self.global_step, 1.0)\n",
    "            \n",
    "            \n",
    "        #############################################################################################\n",
    "        # DEFINE LEARNING RATE FOR TRAINING\n",
    "        #############################################################################################\n",
    "        with tf.name_scope(\"define_weight_decay\"):\n",
    "            moving_ave = tf.train.ExponentialMovingAverage(self.config.wdecay).apply(tf.trainable_variables())\n",
    "            \n",
    "            \n",
    "        #############################################################################################\n",
    "        # DEFINE THE PARAMETERS TO TRAIN\n",
    "        #############################################################################################\n",
    "        \n",
    "        with tf.name_scope(\"define_train_whole\"):\n",
    "            optimizer_train_whole = tf.compat.v1.train.AdamOptimizer(self.learning_rate).minimize(self.loss_whole, var_list=tf.trainable_variables())\n",
    "            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "                with tf.control_dependencies([optimizer_train_whole, global_step_update]):\n",
    "                    with tf.control_dependencies([moving_ave]):\n",
    "                        self.train_op_whole = tf.no_op()\n",
    "\n",
    "        with tf.name_scope(\"define_train_split\"):\n",
    "            optimizer_train_split = tf.compat.v1.train.AdamOptimizer(self.learning_rate).minimize(self.loss_split, var_list=tf.trainable_variables())\n",
    "            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "                with tf.control_dependencies([optimizer_train_split, global_step_update]):\n",
    "                    with tf.control_dependencies([moving_ave]):\n",
    "                        self.train_op_spit = tf.no_op()\n",
    "\n",
    "                    \n",
    "        #############################################################################################\n",
    "        # SUMMARY\n",
    "        #############################################################################################        \n",
    "        with tf.name_scope('summary'):\n",
    "            tf.compat.v1.summary.scalar(\"learning_rate\", self.learning_rate)\n",
    "            tf.compat.v1.summary.scalar(\"Classification_loss\", self.Classification_loss)\n",
    "            tf.compat.v1.summary.scalar(\"Learning_loss_loss_whole\", self.Learning_loss_loss_whole)\n",
    "            tf.compat.v1.summary.scalar(\"Learning_loss_loss_split\", self.Learning_loss_loss_split)\n",
    "            tf.compat.v1.summary.scalar(\"True_Learning_loss_loss\", self.l_true)\n",
    "            tf.compat.v1.summary.scalar(\"Total_loss_slit\", self.loss_split)\n",
    "            tf.compat.v1.summary.scalar(\"Total_loss_whole\", self.loss_whole)\n",
    "            \n",
    "        \n",
    "        self.sess.graph.as_default()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        #self.run_watcher = get_run_watcher()\n",
    "        #self.run_watcher.add_run.remote(name=self.name_run,\n",
    "        #                                user=self.user,\n",
    "        #                                progress=self.progress,\n",
    "        #                                wandb_url=self.wandb.run.get_url(),\n",
    "        #                                status=\"Idle\")\n",
    "\n",
    "                \n",
    "    def get_run_dir(self):\n",
    "        import glob\n",
    "        import os\n",
    "\n",
    "        model_name = self.project.lower().replace(' ', '_')\n",
    "        runs = glob.glob(os.path.join(self.runs_folder, model_name + '_AL_*'))\n",
    "        run_id = len(runs)\n",
    "        return os.path.join(self.runs_folder, model_name + '_AL_' + str(run_id)),  model_name + '_AL_' + str(run_id)\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        self.stop_flag=False\n",
    "        import os\n",
    "        \n",
    "    #try:\n",
    "        import time\n",
    "\n",
    "        \n",
    "        # Make the director to save the trained models\n",
    "        checkpoint_dir = os.path.join(self.run_dir, 'checkpoint')\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.mkdir(checkpoint_dir)\n",
    "            \n",
    "        # load an initial weight\n",
    "        if self.initial_weight is not False:\n",
    "            try:\n",
    "                print('=> Restoring weights from: %s ... ' % os.path.join(checkpoint_dir, self.initial_weight))\n",
    "                self.loader.restore(self.sess, os.path.join(checkpoint_dir, self.initial_weight))\n",
    "            except:\n",
    "                print('=> %s does not exist !!!' % self.initial_weight)\n",
    "                print('=> Now it starts to train from scratch ...')\n",
    "                self.first_stage_epochs = 0\n",
    "        elif self.base_weight_path is not False:\n",
    "                print('=> Restoring weights from: %s ... ' % self.base_weight_path)\n",
    "                self.loader.restore(self.sess, self.base_weight_path)\n",
    "                if self.start_epoch > 0:\n",
    "                    self.first_stage_epochs = 0\n",
    "        else:\n",
    "            print('=> Starts to train from scratch ')\n",
    "\n",
    "        \n",
    "        print('self.steps_per_epoch '+str(self.steps_per_epoch))\n",
    "        #self.run_watcher.update_run.remote(name=self.name_run, status=\"Training\")\n",
    "        for epoch in tqdm(range(self.start_epoch + 1, 1+(self.config.EPOCH_WHOLE +self.config.EPOCH_SLIT ))):\n",
    "            if self.stop_flag:\n",
    "                #self.run_watcher.update_run.remote(name=self.name_run, status=\"Idle\")\n",
    "                break\n",
    "            if epoch <= self.config.EPOCH_WHOLE:\n",
    "                train_op = self.train_op_whole\n",
    "            else:\n",
    "                train_op = self.train_op_split\n",
    "                \n",
    "\n",
    "            train_epoch_loss, test_epoch_loss = [], []\n",
    "            count_nan = 0\n",
    "            for step in tqdm(range(self.steps_per_epoch)):\n",
    "                \n",
    "                if self.stop_flag:\n",
    "                    break\n",
    "                    \n",
    "                # runs the session\n",
    "                if epoch <= self.config.EPOCH_WHOLE:\n",
    "                    # whole model\n",
    "                    _, Classification_loss, Learning_loss_loss, l_true, loss, global_step_val = self.sess.run(\n",
    "                        [train_op, self.Classification_loss, self.Learning_loss_loss_whole, self.l_true, self.loss_whole, self.global_step], feed_dict={self.trainable: True})\n",
    "                else:\n",
    "                    # split model\n",
    "                    _, Classification_loss, Learning_loss_loss, l_true, loss, global_step_val = self.sess.run(\n",
    "                        [train_op, self.Classification_loss, self.Learning_loss_loss_split, self.l_true, self.loss_split, self.global_step], feed_dict={self.trainable: True})\n",
    "                    \n",
    "\n",
    "                # Save the loss\n",
    "                if np.isnan(Classification_loss):\n",
    "                    count_nan += 1\n",
    "                else:\n",
    "                    train_epoch_loss.append(loss)\n",
    "                    \n",
    "                # Set the info for wandb\n",
    "                if step % 50 == 0:\n",
    "                    self.wandb.log({\n",
    "                        'Classification_loss': Classification_loss,\n",
    "                        'Learning_loss_loss': Learning_loss_loss,\n",
    "                        'l_true': l_true,\n",
    "                        'loss': loss,\n",
    "                    })\n",
    "                    \n",
    "                # update global step\n",
    "                self.global_step_val = global_step_val\n",
    "\n",
    "                # Update progress\n",
    "                if step % 100 == 0:\n",
    "                    self.progress = round(self.global_step_val / self.global_step_goal * 100.0, 2)\n",
    "                    #self.run_watcher.update_run.remote(name=self.name_run, progress=self.progress)\n",
    "\n",
    "            # get the mean epoch total loss\n",
    "            train_epoch_loss = np.mean(train_epoch_loss)\n",
    "\n",
    "            \n",
    "            log_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "            print(\"Epoch: %2d Time: %s Train loss: %.2f Count NaN: %d Saving \\n\"%(epoch, log_time, train_epoch_loss, count_nan))\n",
    "            ckpt_file = os.path.join(checkpoint_dir, \"epoch\"+str(epoch)+\".ckpt\")\n",
    "            self.saver.save(self.sess, ckpt_file, global_step=epoch)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            #try:\n",
    "\n",
    "             #   detection_test = local_file(\"test_actor\").YoloTest\n",
    "             #   tester = detection_test.remote(self.project,\n",
    "             #                                   self.source,\n",
    "             #                                   self.dataset_name,\n",
    "             #                                   self.group,\n",
    "             #                                   self.run_dir,\n",
    "             #                                   self.datanet,\n",
    "             #                                   self.dataset_header,\n",
    "             #                                   self.config,\n",
    "             #                                   \"epoch\"+str(epoch)+\".ckpt-\"+str(epoch),\n",
    "             #                                   self.global_step_val,\n",
    "             #                                   wandb_id=self.test_run_id, agnostic_eval=self.agnostic_eval)\n",
    "\n",
    "              #  if self.test_run_id is None:\n",
    "              #      self.test_run_id = ray.get(tester.get_wandb_id.remote())\n",
    "              #  tester.evaluate.remote()\n",
    "              #  tester.__ray_terminate__.remote()\n",
    "            #except Exception as e:\n",
    "            #    print(e)\n",
    "\n",
    "            if self.val_dataset_name is not None and self.val_source is not None:\n",
    "                try:\n",
    "                    detection_val = local_file(\"val_actor\").YoloVal\n",
    "                    validator = detection_val.remote(self.project,\n",
    "                                                    self.val_source,\n",
    "                                                    self.val_dataset_name,\n",
    "                                                    self.group,\n",
    "                                                    self.run_dir,\n",
    "                                                    self.datanet,\n",
    "                                                    self.dataset_header,\n",
    "                                                    self.config,\n",
    "                                                    \"epoch\"+str(epoch)+\".ckpt-\"+str(epoch),\n",
    "                                                    self.global_step_val,\n",
    "                                                    wandb_id=self.val_run_id, agnostic_eval=self.agnostic_eval)\n",
    "\n",
    "                    if self.val_run_id is None:\n",
    "                        self.val_run_id = ray.get(validator.get_wandb_id.remote())\n",
    "                    validator.evaluate.remote()\n",
    "                    validator.__ray_terminate__.remote()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "    #    self.run_watcher.update_run.remote(name=self.name_run, status=\"Finished\", progress=self.progress)\n",
    "    #except Exception as e:\n",
    "    #    self.run_watcher.update_run.remote(name=self.name_run, status=\"Error\")\n",
    "    #    print(e)\n",
    "    \"\"\"\n",
    "if False:\n",
    "    # execute only if run as a script\n",
    "    import random\n",
    "\n",
    "    #############################################################################################\n",
    "    # LOAD DATA\n",
    "    #############################################################################################\n",
    "    from data_utils import CIFAR10Data\n",
    "    # Load data\n",
    "    cifar10_data = CIFAR10Data()\n",
    "    num_classes = len(cifar10_data.classes)\n",
    "    x_train, y_train, x_test, y_test = cifar10_data.get_data(subtract_mean=True)\n",
    "\n",
    "    #############################################################################################\n",
    "    # LOAD CONFIG\n",
    "    #############################################################################################\n",
    "    from config import Load_config\n",
    "    config = Load_config(len(x_train),x_train.shape[1:],num_classes,active_learning = False)\n",
    "\n",
    "    # Initialize a labeled dataset by randomly sampling K=ADDENDUM=1,000 data points from the entire dataset.\n",
    "    indices = list(range(config[\"NUM_TRAIN\"]))\n",
    "    random.shuffle(indices)\n",
    "    labeled_set = indices[:config[\"ADDENDUM\"]]\n",
    "    unlabeled_set = indices[config[\"ADDENDUM\"]:]\n",
    "    \n",
    "    AL_train_cifar_w =  AL_train_cifar(config, labeled_set)\n",
    "    \n",
    "    AL_train_cifar_w.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_agent_cifar import AL_train_cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10 Training data shape: (50000, 32, 32, 3)\n",
      "CIFAR10 Training label shape (50000, 1)\n",
      "CIFAR10 Test data shape (10000, 32, 32, 3)\n",
      "CIFAR10 Test label shape (10000, 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AL_train_cifar_w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9bc0a2e0e360>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mAL_train_cifar_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'AL_train_cifar_w' is not defined"
     ]
    }
   ],
   "source": [
    "# execute only if run as a script\n",
    "import random\n",
    "\n",
    "#############################################################################################\n",
    "# LOAD DATA\n",
    "#############################################################################################\n",
    "from data_utils import CIFAR10Data\n",
    "# Load data\n",
    "cifar10_data = CIFAR10Data()\n",
    "num_classes = len(cifar10_data.classes)\n",
    "x_train, y_train, x_test, y_test = cifar10_data.get_data(subtract_mean=True)\n",
    "\n",
    "#############################################################################################\n",
    "# LOAD CONFIG\n",
    "#############################################################################################\n",
    "from config import Load_config\n",
    "config = Load_config(len(x_train),x_train.shape[1:],num_classes,active_learning = False)\n",
    "\n",
    "# Initialize a labeled dataset by randomly sampling K=ADDENDUM=1,000 data points from the entire dataset.\n",
    "indices = list(range(config[\"NUM_TRAIN\"]))\n",
    "random.shuffle(indices)\n",
    "labeled_set = indices[:config[\"ADDENDUM\"]]\n",
    "unlabeled_set = indices[config[\"ADDENDUM\"]:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10 Training data shape: (50000, 32, 32, 3)\n",
      "CIFAR10 Training label shape (50000, 1)\n",
      "CIFAR10 Test data shape (10000, 32, 32, 3)\n",
      "CIFAR10 Test label shape (10000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mafospinat\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.11<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">royal-eon-16</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/afospinat/AL_CIFAR\" target=\"_blank\">https://wandb.ai/afospinat/AL_CIFAR</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/afospinat/AL_CIFAR/runs/28hmv2fh\" target=\"_blank\">https://wandb.ai/afospinat/AL_CIFAR/runs/28hmv2fh</a><br/>\n",
       "                Run data is saved locally in <code>/workspace/Active Learning/wandb/run-20201129_185731-28hmv2fh</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "fles =  AL_train_cifar(config, labeled_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
