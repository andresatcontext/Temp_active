{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load defaulft config\n",
    "import yaml\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "config_path = './configs/default_cifar.yml'\n",
    "\n",
    "with open(config_path) as file:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    \n",
    "\n",
    "# create base dir and gr\n",
    "if os.path.exists(config[\"PROJECT\"][\"project_dir\"]) is False:\n",
    "    os.mkdir(config[\"PROJECT\"][\"project_dir\"])\n",
    "\n",
    "if os.path.exists(config[\"PROJECT\"][\"group_dir\"]) is False:\n",
    "    os.mkdir(config[\"PROJECT\"][\"group_dir\"])\n",
    "    \n",
    "    \n",
    "# Get the data to annotate\n",
    "\n",
    "#############################################################################################\n",
    "# LOAD DATA\n",
    "#############################################################################################\n",
    "from data_utils import CIFAR10Data\n",
    "# Load data\n",
    "cifar10_data = CIFAR10Data()\n",
    "num_classes = len(cifar10_data.classes)\n",
    "x_train, y_train, x_test, y_test = cifar10_data.get_data(normalize_data=False)\n",
    "\n",
    "indices = list(range(len(x_train)))\n",
    "random.seed(101)\n",
    "random.shuffle(indices)\n",
    "labeled_set = indices\n",
    "unlabeled_set =[]\n",
    "\n",
    "\n",
    "# test with all the images\n",
    "NUM_IMAGES_TEST = len(x_test)\n",
    "# Initialize a labeled dataset by randomly sampling K=ADDENDUM=1,000 data points from the entire dataset.\n",
    "test_set = list(range(NUM_IMAGES_TEST))\n",
    "\n",
    "config[\"NETWORK\"][\"INPUT_SIZE\"] =  x_train[0].shape[0]\n",
    "config[\"NETWORK\"][\"CLASSES\"] = cifar10_data.classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACTIVE_ALGO': {'LOSSLEARNING': 1.0},\n",
       " 'DATASET': {'height_shift_range': 4,\n",
       "  'horizontal_flip': True,\n",
       "  'width_shift_range': 4},\n",
       " 'NETWORK': {'CLASSES': ['plane',\n",
       "   'car',\n",
       "   'bird',\n",
       "   'cat',\n",
       "   'deer',\n",
       "   'dog',\n",
       "   'frog',\n",
       "   'horse',\n",
       "   'ship',\n",
       "   'truck'],\n",
       "  'INPUT_SIZE': 32,\n",
       "  'MARGIN': 1.0,\n",
       "  'embedding_size': 128},\n",
       " 'PROJECT': {'Backbone': 'resnet18',\n",
       "  'dataset_name': 'CIFAR',\n",
       "  'group': 'Classif_all_data_0912',\n",
       "  'group_dir': '/mnt/Ressources/Andres/Temp_active/runs/Classif_all_data_0912',\n",
       "  'project': 'Active_Learning_CIFAR',\n",
       "  'project_dir': '/mnt/Ressources/Andres/Temp_active/runs',\n",
       "  'source': 'CIFAR'},\n",
       " 'RUNS': {'ADDENDUM': 1000,\n",
       "  'CYCLES': 1,\n",
       "  'SUBSET': -1,\n",
       "  'TRIALS': 1,\n",
       "  'test_each': 1},\n",
       " 'TEST': {'batch_size': 128},\n",
       " 'TRAIN': {'Data_augementation': True,\n",
       "  'EPOCH_SLIT': 20,\n",
       "  'EPOCH_WARMUP': 2,\n",
       "  'EPOCH_WHOLE': 40,\n",
       "  'MILESTONES': [25, 35],\n",
       "  'batch_size': 128,\n",
       "  'gamma': 0.1,\n",
       "  'lr': 0.01,\n",
       "  'start_epoch': 0,\n",
       "  'transfer_weight_path': False,\n",
       "  'w_c_loss': 1.0,\n",
       "  'w_l_loss': 0,\n",
       "  'wdecay': 0.995}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Active_Learning_train:\n",
    "    def __init__(self,   config, \n",
    "                         labeled_set,\n",
    "                         test_set, \n",
    "                         num_run,\n",
    "                         resume_model_path,\n",
    "                         resume = False):\n",
    "\n",
    "        \n",
    "        #############################################################################################\n",
    "        # LIBRARIES\n",
    "        #############################################################################################        \n",
    "        import os\n",
    "        \"\"\"\n",
    "        # dont work in notebook\n",
    "        self.run_path = os.path.dirname(os.path.realpath(__file__))\n",
    "        os.chdir(self.run_path)\n",
    "        # stuff using ray\n",
    "        core = local_module(\"core\")\n",
    "        backbones = local_module(\"backbones\")\n",
    "        self.user            = get_user()\n",
    "        \"\"\"\n",
    "        import core\n",
    "        import backbones\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        from tensorflow.python import pywrap_tensorflow\n",
    "        import numpy as np\n",
    "        from tensorflow.keras import optimizers, losses, models, backend, layers, metrics\n",
    "        \n",
    "\n",
    "        #############################################################################################\n",
    "        # SETUP TENSORFLOW SESSION\n",
    "        #############################################################################################\n",
    "        config_tf = tf.ConfigProto(allow_soft_placement=True) \n",
    "        config_tf.gpu_options.allow_growth = True \n",
    "        self.sess = tf.Session(config=config_tf)\n",
    "        self.sess.graph.as_default()\n",
    "        backend.set_session(self.sess)\n",
    "\n",
    "    \n",
    "        #############################################################################################\n",
    "        # PARAMETERS RUN\n",
    "        #############################################################################################\n",
    "        \n",
    "        self.config          = config\n",
    "        self.num_run         = num_run\n",
    "        self.group           = \"Stage_\"+str(num_run)\n",
    "        self.name_run        = \"Train_\"+self.group \n",
    "        \n",
    "        self.run_dir         = os.path.join(config[\"PROJECT\"][\"group_dir\"],self.group)\n",
    "        self.run_dir_check   = os.path.join(self.run_dir ,'checkpoints')\n",
    "        self.checkpoints_path= os.path.join(self.run_dir_check,'checkpoint.{epoch:03d}.hdf5')\n",
    "        self.test_run_id     = None\n",
    "        self.stop_flag       = False\n",
    "        self.training_thread = None\n",
    "        self.resume_training = resume\n",
    "        \n",
    "        self.num_data_train  = len(labeled_set) \n",
    "        self.resume_model_path = resume_model_path\n",
    "        self.transfer_weight_path = self.config['TRAIN'][\"transfer_weight_path\"]\n",
    "        self.num_class       = len(self.config[\"NETWORK\"][\"CLASSES\"])\n",
    "        self.input_shape     = [self.config[\"NETWORK\"][\"INPUT_SIZE\"], self.config[\"NETWORK\"][\"INPUT_SIZE\"], 3]\n",
    "        \n",
    "        \n",
    "        self.pre ='\\033[1;36m' + self.name_run + '\\033[0;0m' #\"____\" #\n",
    "        self.problem ='\\033[1;31m' + self.name_run + '\\033[0;0m'\n",
    "        \n",
    "        # Creating the train folde\n",
    "        import shutil\n",
    "        \n",
    "        if os.path.exists(self.run_dir) and self.resume_model_path is False:\n",
    "            if num_run==0:\n",
    "                shutil.rmtree(config[\"PROJECT\"][\"group_dir\"])\n",
    "                os.mkdir(config[\"PROJECT\"][\"group_dir\"])\n",
    "            else:  \n",
    "                shutil.rmtree(self.run_dir)\n",
    "                \n",
    "        if os.path.exists(self.run_dir) is False:\n",
    "            os.mkdir(self.run_dir)\n",
    "            \n",
    "        if os.path.exists(self.run_dir_check) is False:\n",
    "            os.mkdir(self.run_dir_check)\n",
    "\n",
    "            \n",
    "        #############################################################################################\n",
    "        # SETUP WANDB\n",
    "        #############################################################################################\n",
    "        \"\"\"\n",
    "        import wandb\n",
    "        \n",
    "        self.wandb = wandb\n",
    "        self.wandb.init(project  = config[\"PROJECT\"][\"project\"], \n",
    "                        group    = config[\"PROJECT\"][\"group\"], \n",
    "                        name     = \"Train_\"+str(num_run),\n",
    "                        job_type = self.group ,\n",
    "                        sync_tensorboard = True,\n",
    "                        config = config)\n",
    "        \"\"\"\n",
    "\n",
    "        #############################################################################################\n",
    "        # GLOBAL PROGRESS\n",
    "        #############################################################################################\n",
    "        self.current_epoch = 0\n",
    "        self.split_epoch   = self.config['TRAIN'][\"EPOCH_WHOLE\"] \n",
    "        self.total_epochs  = self.config['TRAIN'][\"EPOCH_WHOLE\"] + self.config['TRAIN'][\"EPOCH_SLIT\"]\n",
    "        self.progress = round(self.current_epoch / self.total_epochs * 100.0, 2)\n",
    "\n",
    "        #############################################################################################\n",
    "        # LOAD DATA\n",
    "        #############################################################################################\n",
    "        if self.config[\"PROJECT\"][\"source\"]=='CIFAR':\n",
    "            from data_utils import CIFAR10Data\n",
    "            # Load data\n",
    "            cifar10_data = CIFAR10Data()\n",
    "            x_train, y_train, _, _ = cifar10_data.get_data(normalize_data=False)\n",
    "\n",
    "            x_train = x_train[labeled_set]\n",
    "            y_train = y_train[labeled_set]\n",
    "            \n",
    "            self.test_set = test_set\n",
    "        else:\n",
    "            raise NameError('This is not implemented yet')\n",
    "        \n",
    "        \n",
    "        #############################################################################################\n",
    "        # DATA GENERATOR\n",
    "        #############################################################################################\n",
    "        self.Data_Generator = core.Generator_cifar_train(x_train, y_train, config)\n",
    "\n",
    "\n",
    "        #############################################################################################\n",
    "        # GENERATE MODEL\n",
    "        #############################################################################################\n",
    "\n",
    "        \"\"\"\n",
    "        ResNet18\n",
    "        ResNet50\n",
    "        ResNet101\n",
    "        ResNet152\n",
    "        ResNet50V2\n",
    "        ResNet101V2\n",
    "        ResNet152V2\n",
    "        ResNeXt50\n",
    "        ResNeXt101\n",
    "        \"\"\"\n",
    "        #############################################################################################\n",
    "        # DEFINE CLASSIFIER\n",
    "        #############################################################################################\n",
    "        # set input\n",
    "        img_input = tf.keras.Input(self.input_shape,name= 'input_image')\n",
    "\n",
    "        include_top = True\n",
    "\n",
    "        # Get the selected backbone\n",
    "        self.backbone = getattr(backbones,\"ResNet18_cifar\")\n",
    "        #\n",
    "        c_pred_features = self.backbone(input_tensor=img_input, classes= self.num_class, include_top=include_top)\n",
    "        self.c_pred_features= c_pred_features\n",
    "        if include_top: # include top classifier\n",
    "            # class predictions\n",
    "            c_pred = c_pred_features[0]\n",
    "        else:\n",
    "            x = layers.GlobalAveragePooling2D(name='pool1')(c_pred_features[0])\n",
    "            x = layers.Dense(self.num_class, name='fc1')(x)\n",
    "            c_pred = layers.Activation('softmax', name='c_pred')(x)\n",
    "            c_pred_features[0]=c_pred\n",
    "\n",
    "        self.classifier = models.Model(inputs=[img_input], outputs=c_pred_features,name='Classifier') \n",
    "\n",
    "        #############################################################################################\n",
    "        # DEFINE FULL MODEL\n",
    "        #############################################################################################\n",
    "        c_pred_features_1 = self.classifier(img_input)\n",
    "        c_pred_1 = c_pred_features_1[0]\n",
    "\n",
    "        # define lossnet\n",
    "        loss_pred_embeddings = core.Lossnet(c_pred_features_1, self.config[\"NETWORK\"][\"embedding_size\"])\n",
    "\n",
    "        self.model = models.Model(inputs=img_input, outputs=[c_pred_1]+loss_pred_embeddings) #, embedding_s] )\n",
    "        \n",
    "        #############################################################################################\n",
    "        # DEFINE LOSSES\n",
    "        #############################################################################################\n",
    "        # losses\n",
    "        self.loss_dict = {}\n",
    "        self.loss_dict['Classifier'] = losses.categorical_crossentropy\n",
    "        self.loss_dict['l_pred_w']   = core.Loss_Lossnet\n",
    "        self.loss_dict['l_pred_s']   = core.Loss_Lossnet\n",
    "        # weights\n",
    "        self.weight_w = backend.variable(1)\n",
    "        self.weight_s = backend.variable(0)\n",
    "        \n",
    "        self.loss_w_dict = {}\n",
    "        self.loss_w_dict['Classifier'] = 1\n",
    "        self.loss_w_dict['l_pred_w']   = self.weight_w\n",
    "        self.loss_w_dict['l_pred_s']   = self.weight_s\n",
    "        self.loss_w_dict['Embedding']  = 0\n",
    "        \n",
    "        #############################################################################################\n",
    "        # DEFINE METRICS\n",
    "        #############################################################################################\n",
    "        # metrics\n",
    "        self.metrics_dict = {}\n",
    "        self.metrics_dict['Classifier'] = metrics.categorical_accuracy\n",
    "        self.metrics_dict['l_pred_w']   = core.MAE_Lossnet\n",
    "        self.metrics_dict['l_pred_s']   = core.MAE_Lossnet\n",
    "        \n",
    "        #############################################################################################\n",
    "        # DEFINE OPTIMIZER\n",
    "        #############################################################################################\n",
    "        self.opt = optimizers.Adam(lr=0.01)\n",
    "        \n",
    "        #############################################################################################\n",
    "        # DEFINE CALLBACKS\n",
    "        #############################################################################################\n",
    "        # Checkpoint saver\n",
    "        self.callbacks = []\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                filepath=self.checkpoints_path,\n",
    "                                                save_weights_only=True,\n",
    "                                                period=self.config[\"RUNS\"][\"test_each\"])\n",
    "        \n",
    "        \n",
    "        self.callbacks.append(model_checkpoint_callback)\n",
    "        \n",
    "        # Callback to wandb\n",
    "        #self.callbacks.append(self.wandb.keras.WandbCallback())\n",
    "        \n",
    "        # Callback Learning Rate\n",
    "        def scheduler(epoch):\n",
    "            lr = self.config['TRAIN']['lr']\n",
    "            for i in self.config['TRAIN']['MILESTONES']:\n",
    "                if epoch>i:\n",
    "                    lr*=0.1\n",
    "            return lr\n",
    "        \n",
    "        self.callbacks.append(tf.keras.callbacks.LearningRateScheduler(scheduler))\n",
    "        \n",
    "        # callbeck to change the weigths for the split training:\n",
    "        self.callbacks.append(core.Change_loss_weights(self.weight_w, self.weight_s, 3))\n",
    "        \n",
    "        #############################################################################################\n",
    "        # LOAD PREVIUS WEIGTHS\n",
    "        #############################################################################################\n",
    "        if self.resume_model_path:\n",
    "            # check the epoch where is loaded\n",
    "            try:\n",
    "                loaded_epoch = int(self.resume_model_path.split('.')[-2])\n",
    "                print(self.pre, \"Loading weigths from: \",self.resume_model_path)\n",
    "                print(self.pre, \"The detected epoch is: \",loaded_epoch)\n",
    "                # load weigths\n",
    "                self.model.load_weights(self.resume_model_path)\n",
    "            except:\n",
    "                print( self.problem ,\"=> Problem loading the weights from \",self.resume_model_path)\n",
    "                print( self.problem ,'=> It will rain from scratch')\n",
    "                \n",
    "        if self.resume_training:\n",
    "            self.current_epoch = loaded_epoch\n",
    "            self.progress = round(self.current_epoch / self.total_epochs * 100.0, 2)\n",
    "            \n",
    "            if self.current_epoch > self.total_epochs:\n",
    "                raise ValueError(\"The starting epoch is higher that the total epochs\")\n",
    "            else:\n",
    "                print(self.pre, \"Resuming the training from stage: \",self.num_run,\" at epoch \", self.current_epoch)\n",
    "\n",
    "        #############################################################################################\n",
    "        # COMPILE MODEL\n",
    "        #############################################################################################        \n",
    "        self.model.compile(loss = self.loss_dict, \n",
    "                           loss_weights = self.loss_w_dict, \n",
    "                           metrics = self.metrics_dict, \n",
    "                          optimizer = self.opt)\n",
    "    \n",
    "        #############################################################################################\n",
    "        # INIT VARIABLES\n",
    "        #############################################################################################\n",
    "        self.sess.graph.as_default()\n",
    "        backend.set_session(self.sess)\n",
    "        self.sess.run(tf.local_variables_initializer())\n",
    "\n",
    "        #############################################################################################\n",
    "        # SETUP WATCHER\n",
    "        #############################################################################################    \n",
    "    \"\"\"\n",
    "        self.run_watcher = get_run_watcher()\n",
    "        \n",
    "        self.run_watcher.add_run.remote(name=self.name_run,\n",
    "                                        user=self.user,\n",
    "                                        progress=self.progress,\n",
    "                                        wandb_url=self.wandb.run.get_url(),\n",
    "                                        status=\"Idle\")\n",
    "        print(self.pre,'Init done')\n",
    "        \n",
    "\n",
    "    \n",
    "    @ray.method(num_returns = 0)\n",
    "    def start_training(self):\n",
    "        import threading\n",
    "        import os\n",
    "        import numpy as np\n",
    "        from copy import deepcopy\n",
    "        from tensorflow.keras import backend\n",
    "        \n",
    "        def train():\n",
    "            try:\n",
    "                self.sess.graph.as_default()\n",
    "                backend.set_session(self.sess)\n",
    "                #self.sess.run(tf.local_variables_initializer())\n",
    "                \n",
    "                print( self.pre ,\"Start training\")\n",
    "                self.run_watcher.update_run.remote(name=self.name_run, status=\"Training\")\n",
    "                \n",
    "                ###############################################################################\n",
    "                # TRAIN THE WHOLE NETWORK\n",
    "                ###############################################################################\n",
    "                if self.current_epoch <= self.split_epoch: \n",
    "                    \n",
    "                    print( self.pre ,\"Compile with new weights for the losses\")\n",
    "                    # change the weigth to the predictions of the whole network\n",
    "                    #self.loss_w_dict['l_pred_w']   = 1\n",
    "                    #self.loss_w_dict['l_pred_s']   = 0\n",
    "                    \n",
    "\n",
    "                    print( self.pre ,\"End compiling\")\n",
    "                    \n",
    "                    self.sess.run(tf.local_variables_initializer())\n",
    "                    print( self.pre ,\"Init local\")\n",
    "                    # run epoch by epoch to be able to have the stop flag\n",
    "                    for epoch in range(self.current_epoch, self.split_epoch):\n",
    "                        \n",
    "                        print( self.pre ,\"Training epoch\", epoch)\n",
    "                        \n",
    "                        if self.stop_flag:\n",
    "                            self.run_watcher.update_run.remote(name=self.name_run, status=\"Idle\")\n",
    "                            break\n",
    "                        \n",
    "                        history = self.model.fit_generator(self.Data_Generator,\n",
    "                                                           epochs=epoch+1, \n",
    "                                                           callbacks = self.callbacks,\n",
    "                                                           initial_epoch=epoch,\n",
    "                                                           verbose=1)\n",
    "\n",
    "   \n",
    "                        self.current_epoch = epoch\n",
    "                        self.progress = round(self.current_epoch / self.total_epochs * 100.0, 2)\n",
    "                        self.run_watcher.update_run.remote(name=self.name_run, progress=self.progress)\n",
    "\n",
    "                    \n",
    "                if self.current_epoch <= self.total_epochs:\n",
    "                    print( self.pre ,\"Compile with new weights for the losses\")\n",
    "                    # change the weigth to the predictions of the whole network\n",
    "                    self.loss_w_dict['l_pred_w']   = 0\n",
    "                    self.loss_w_dict['l_pred_s']   = 1\n",
    "                    \n",
    "                    # compile the model\n",
    "                    self.model.compile(loss = self.loss_dict, \n",
    "                           loss_weights = self.loss_w_dict, \n",
    "                           metrics = self.metrics_dict, \n",
    "                           optimizer = self.opt)\n",
    "                    \n",
    "                    self.sess.run(tf.local_variables_initializer())\n",
    "                    # run epoch by epoch to be able to have the stop flag\n",
    "                    for epoch in range(self.current_epoch, self.total_epochs):\n",
    "                        print( self.pre ,\"Training epoch\", epoch)\n",
    "\n",
    "                        if self.stop_flag:\n",
    "                            self.run_watcher.update_run.remote(name=self.name_run, status=\"Idle\")\n",
    "                            break\n",
    "                        \n",
    "                        history = self.model.fit_generator(self.Data_Generator,\n",
    "                                                           epochs=epoch+1, \n",
    "                                                           callbacks = self.callbacks,\n",
    "                                                           initial_epoch=epoch ,verbose=1)\n",
    "\n",
    "   \n",
    "                        self.current_epoch = epoch\n",
    "                        self.progress = round(self.current_epoch / self.total_epochs * 100.0, 2)\n",
    "                        self.run_watcher.update_run.remote(name=self.name_run, progress=self.progress)\n",
    "                if self.current_epoch > self.total_epochs:\n",
    "                    print(self.problem, 'The starting epoch is higher that the total epochs')\n",
    "                self.run_watcher.update_run.remote(name=self.name_run, status=\"Finished\", progress=self.progress)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.run_watcher.update_run.remote(name=self.name_run, status=\"Error\")\n",
    "                print( self.problem ,e)\n",
    "            \n",
    "        if self.training_thread is None or not self.training_thread.is_alive():\n",
    "            self.stop_flag=False\n",
    "            self.training_thread = threading.Thread(target=train, args=(), daemon=True)\n",
    "            self.training_thread.start()\n",
    "            \n",
    "    @ray.method(num_returns = 1)\n",
    "    def isTraining(self):\n",
    "        return not (self.training_thread is None or not self.training_thread.is_alive())\n",
    "\n",
    "    @ray.method(num_returns = 0)\n",
    "    def stop_training(self):\n",
    "        self.stop_flag=True\n",
    "\n",
    "    @ray.method(num_returns = 1)\n",
    "    def get_progress(self):\n",
    "        return {\"global_step\" : self.global_step_val, \"progress\": self.progress, }\n",
    "        \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:Output \"Embedding\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"Embedding\".\n",
      "WARNING:tensorflow:Output \"Embedding\" missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to \"Embedding\".\n",
      "WARNING:tensorflow:From /home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/ops/metrics_impl.py:363: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "resume_model_path = '/mnt/Ressources/Andres/Temp_active/runs/Classif_all_data_0912/Stage_5000/checkpoints/checkpoint.002.hdf5'\n",
    "resume_model_path = False\n",
    "fles = Active_Learning_train(config,labeled_set,[],5000,resume_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers, losses, models, backend, layers, metrics\n",
    "import tensorflow as tf\n",
    "import core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36mTrain_Stage_5000\u001b[0;0m Start training\n",
      "WARNING:tensorflow:From /home/reminiz/ReminizML2/python_env/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/6\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.9405 - Classifier_loss: 2.0388 - l_pred_w_loss: 0.9015 - l_pred_s_loss: 0.9015 - Classifier_categorical_accuracy: 0.3140 - l_pred_w_MAE_Lossnet: 2.6884 - l_pred_s_MAE_Lossnet: 2.6884in <tf.Variable 'Variable:0' shape=() dtype=float32> <tf.Variable 'Variable_1:0' shape=() dtype=float32>\n",
      "out 1 0\n",
      "391/391 [==============================] - 40s 101ms/step - loss: 2.9400 - Classifier_loss: 2.0383 - l_pred_w_loss: 0.9015 - l_pred_s_loss: 0.9015 - Classifier_categorical_accuracy: 0.3141 - l_pred_w_MAE_Lossnet: 2.6888 - l_pred_s_MAE_Lossnet: 2.6888\n",
      "Epoch 2/6\n",
      "390/391 [============================>.] - ETA: 0s - loss: 2.3375 - Classifier_loss: 1.5610 - l_pred_w_loss: 0.7766 - l_pred_s_loss: 0.7766 - Classifier_categorical_accuracy: 0.4412 - l_pred_w_MAE_Lossnet: 2.9782 - l_pred_s_MAE_Lossnet: 2.9782in 1 0\n",
      "out 1 0\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 2.3369 - Classifier_loss: 1.5606 - l_pred_w_loss: 0.7764 - l_pred_s_loss: 0.7764 - Classifier_categorical_accuracy: 0.4413 - l_pred_w_MAE_Lossnet: 2.9782 - l_pred_s_MAE_Lossnet: 2.9782\n",
      "Epoch 3/6\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.8975 - Classifier_loss: 1.2326 - l_pred_w_loss: 0.6648 - l_pred_s_loss: 0.6648 - Classifier_categorical_accuracy: 0.5565 - l_pred_w_MAE_Lossnet: 3.0529 - l_pred_s_MAE_Lossnet: 3.0529in 1 0\n",
      "out 1 0\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 1.8969 - Classifier_loss: 1.2324 - l_pred_w_loss: 0.6646 - l_pred_s_loss: 0.6646 - Classifier_categorical_accuracy: 0.5567 - l_pred_w_MAE_Lossnet: 3.0529 - l_pred_s_MAE_Lossnet: 3.0529\n",
      "Epoch 4/6\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.6086 - Classifier_loss: 1.0428 - l_pred_w_loss: 0.5658 - l_pred_s_loss: 0.5658 - Classifier_categorical_accuracy: 0.6298 - l_pred_w_MAE_Lossnet: 3.1160 - l_pred_s_MAE_Lossnet: 3.1160in 1 0\n",
      "out 0 1\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 1.6082 - Classifier_loss: 1.0425 - l_pred_w_loss: 0.5657 - l_pred_s_loss: 0.5657 - Classifier_categorical_accuracy: 0.6299 - l_pred_w_MAE_Lossnet: 3.1161 - l_pred_s_MAE_Lossnet: 3.1161\n",
      "Epoch 5/6\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.4435 - Classifier_loss: 0.9359 - l_pred_w_loss: 0.5077 - l_pred_s_loss: 0.5077 - Classifier_categorical_accuracy: 0.6701 - l_pred_w_MAE_Lossnet: 3.1960 - l_pred_s_MAE_Lossnet: 3.1960in 0 1\n",
      "out 0 1\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 1.4431 - Classifier_loss: 0.9357 - l_pred_w_loss: 0.5074 - l_pred_s_loss: 0.5074 - Classifier_categorical_accuracy: 0.6702 - l_pred_w_MAE_Lossnet: 3.1961 - l_pred_s_MAE_Lossnet: 3.1961\n",
      "Epoch 6/6\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.2964 - Classifier_loss: 0.8422 - l_pred_w_loss: 0.4542 - l_pred_s_loss: 0.4542 - Classifier_categorical_accuracy: 0.7040 - l_pred_w_MAE_Lossnet: 3.3007 - l_pred_s_MAE_Lossnet: 3.3007in 0 1\n",
      "out 0 1\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 1.2957 - Classifier_loss: 0.8421 - l_pred_w_loss: 0.4537 - l_pred_s_loss: 0.4537 - Classifier_categorical_accuracy: 0.7040 - l_pred_w_MAE_Lossnet: 3.3008 - l_pred_s_MAE_Lossnet: 3.3008\n"
     ]
    }
   ],
   "source": [
    "print( fles.pre ,\"Start training\")\n",
    "\n",
    "\n",
    "history = fles.model.fit_generator(fles.Data_Generator,\n",
    "                                   epochs=6, \n",
    "                                   callbacks = fles.callbacks,\n",
    "                                   initial_epoch=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
