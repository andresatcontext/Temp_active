total_loss_tracker = tf.keras.metrics.Mean(name="total_loss")
class_loss_tracker = tf.keras.metrics.Mean(name="class_loss")
loss_loss_tracker  = tf.keras.metrics.Mean(name="loss_loss")

metrics_loss = tf.keras.metrics.RootMeanSquaredError(name="m_loss")
metrics_class = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1,name="m_class")

class Classification_with_AL(tf.keras.Model):
    
    custom_margin = wandb.config.margin
    custom_w_classif_loss = wandb.config.w_classif_loss
    custom_w_loss_loss = wandb.config.w_loss_loss
    custom_reduction_in_loss = wandb.config.reduction_in_loss
        
    def train_step(self, data):
        x, c_true = data

        with tf.GradientTape() as tape:
            y_pred = self(x, training=True)  # Forward pass
            # Compute our own loss
            with tf.name_scope("loss"):
                c_pred = y_pred[0]
                l_pred = y_pred[2]
                with tf.name_scope("Loss_learning_loss") :
                    # loss learning loss
                    loss_loss, l_true = compute_loss_loss(c_true, c_pred, l_pred, margin=self.custom_margin , reduction=self.custom_reduction_in_loss)
                with tf.name_scope("Classification_loss") :
                    # Classification Loss
                    class_loss = tf.keras.losses.SparseCategoricalCrossentropy(c_true, c_pred)
                loss = (self.custom_w_loss_loss * loss_loss) + (self.custom_w_classif_loss * class_loss) 
            

        # Compute gradients
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)

        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        
        # losses mean
        total_loss_tracker.update_state(loss)
        class_loss_tracker.update_state(class_loss)
        loss_loss_tracker.update_state(loss_loss)
        # Compute metrics
        metrics_class.update_state(c_true, c_pred)
        metrics_loss.update_state(l_true, l_pred)
             
        output = {}
        output['total_loss'] = total_loss_tracker.result()
        output['class_loss'] = class_loss_tracker.result()
        output['loss_loss'] = loss_loss_tracker.result()
        output['m_class'] = metrics_class.result()
        output['m_loss'] = metrics_loss.result()
        
        return output

    @property
    def metrics(self):
        # We list our `Metric` objects here so that `reset_states()` can be
        # called automatically at the start of each epoch
        # or at the start of `evaluate()`.
        # If you don't implement this property, you have to call
        # `reset_states()` yourself at the time of your choosing.
        return [total_loss_tracker, class_loss_tracker, loss_loss_tracker,metrics_class,metrics_loss]
        
        
 # Loss Prediction Loss
def LossPredLoss(l_pred, target, margin=1.0, reduction='mean'):
    assert len(l_pred) % 2 == 0, 'the batch size is not even.'
    assert l_pred.shape == l_pred.flip(0).shape
    
    l_pred = (l_pred - l_pred.flip(0))[:len(l_pred)//2] # [l_1 - l_2B, l_2 - l_2B-1, ... , l_B - l_B+1], where batch_size = 2B
    target = (target - target.flip(0))[:len(target)//2]
    target = target.detach()

    one = 2 * torch.sign(torch.clamp(target, min=0)) - 1 # 1 operation which is defined by the authors
    
    if reduction == 'mean':
        loss = torch.sum(torch.clamp(margin - one * l_pred, min=0))
        loss = loss / l_pred.size(0) # Note that the size of l_pred is already halved
    elif reduction == 'none':
        loss = torch.clamp(margin - one * l_pred, min=0)
    else:
        NotImplementedError()
    
    return loss

